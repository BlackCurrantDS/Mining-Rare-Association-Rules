{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1_a)Generate_Rules_Apriori.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCI6PUw5YZjuKYN5UoZkkc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackCurrantDS/DBSE_Project/blob/main/Part1_a)Generate_Rules_Apriori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WlDwmSxcYX7"
      },
      "source": [
        "This Notebook takes the input data and runs apriori algorithm with min support, min confidence as arguments and generated the frequent item sets files and generated association rules.\r\n",
        "\r\n",
        "How to run:\r\n",
        "\r\n",
        "Step 1: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72B5oEdPUMH"
      },
      "source": [
        "class RuleMiner(object):    \r\n",
        "    '''\r\n",
        "    This class is used to generate_itemsets_and_rules and store a Naive Belief System \r\n",
        "    by using the most confident association rules\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, filter_name, train_data_set):\r\n",
        "        \r\n",
        "        self.nthreads = 4\r\n",
        "        self.files_info = ARMFiles(\"/content/\")\r\n",
        "        \r\n",
        "        self.filter_name = filter_name\r\n",
        "        self.data_set = train_data_set #taking input the dataset\r\n",
        "\r\n",
        "\r\n",
        "    '''\r\n",
        "    Generate association rules and select K patterns with highest confidence.\r\n",
        "    '''    \r\n",
        "    def generate_itemsets_and_rules(self, arm_params):\r\n",
        "        self.generate_frequent_itemsets(arm_params)\r\n",
        "        self.generate_association_rules(arm_params)\r\n",
        "        #self.extract_features_4_all_rules()\r\n",
        "\r\n",
        "    '''\r\n",
        "    Generate frequent itemsets from data-set\r\n",
        "    '''\r\n",
        "    def generate_frequent_itemsets(self, arm_params):\r\n",
        "        \r\n",
        "        print ('generating frequent item-sets...')\r\n",
        "        apriori = Apriori(self.data_set)\r\n",
        "        apriori.generate_frequent_itemsets_vw(arm_params.min_sup * self.data_set.size(), \r\n",
        "                                              self.nthreads, \r\n",
        "                                              arm_params.itemset_max_size, \r\n",
        "                                              self.files_info.itemset_tmp_file)\r\n",
        "        \r\n",
        "    '''\r\n",
        "    Generate association rules from data-set. \r\n",
        "    This method must be called after generate_frequent_itemsets(...) is called\r\n",
        "    '''\r\n",
        "    def generate_association_rules(self, arm_params):\r\n",
        "        freq_itemsets_dict = self.load_frequent_itemsets_as_dict()\r\n",
        "        \r\n",
        "        print ('generating rules ....')\r\n",
        "        itemset_formatter = getattr(ItemsetFormatter, self.filter_name)\r\n",
        "        rule_formatter = getattr(RuleFormatter, self.filter_name)\r\n",
        "        rule_generator = Generator(freq_itemsets_dict, \r\n",
        "                                   arm_params.min_conf, \r\n",
        "                                   itemset_formatter, \r\n",
        "                                   rule_formatter, \r\n",
        "                                   self.nthreads)\r\n",
        "        rule_generator.execute(self.files_info.rules_tmp_file)\r\n",
        "\r\n",
        "\r\n",
        "    '''\r\n",
        "    Load generated frequent itemsets from file. \r\n",
        "    This method must be called after generate_frequent_itemsets is called\r\n",
        "    '''\r\n",
        "    def load_frequent_itemsets_as_dict(self):\r\n",
        "        freq_itemset_dict = ItemsetDictionary(0)\r\n",
        "        freq_itemset_dict.load_from_file(self.files_info.itemset_tmp_file)\r\n",
        "        return freq_itemset_dict"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9nUg9WlQ_zE"
      },
      "source": [
        "Helper classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUdyZosYZ5Jx"
      },
      "source": [
        "def string_2_itemset(key):\r\n",
        "    if key == '':\r\n",
        "        return []\r\n",
        "    else: \r\n",
        "        return key.split(',')\r\n",
        "\r\n",
        "def itemset_2_string(itemset):\r\n",
        "    return \",\".join(itemset)"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rb1z5_8RBak"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "class HashItem:\r\n",
        "    \r\n",
        "    def __init__(self, item):\r\n",
        "        self.last_item = item \r\n",
        "        self.tids = []\r\n",
        "    \r\n",
        "    def add_tid(self, tid):\r\n",
        "        self.tids.append(tid)\r\n",
        "        \r\n",
        "    def add_tids(self, tids):\r\n",
        "        self.tids.extend(tids)\r\n",
        "    \r\n",
        "    def size(self):\r\n",
        "        return len(self.tids)\r\n",
        "    \r\n",
        "    def serialize(self):\r\n",
        "        return json.dumps((self.last_item, self.tids))\r\n",
        "    \r\n",
        "    def deserialize(self, json_string):\r\n",
        "        result = json.loads(json_string)\r\n",
        "        self.last_item = result[0]\r\n",
        "        self.tids = result[1]\r\n",
        "        "
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iT9tO_VRDNj"
      },
      "source": [
        "class HashItemCollection:\r\n",
        "    def __init__(self):\r\n",
        "        self.train_data = []\r\n",
        "    \r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.train_data)\r\n",
        "    \r\n",
        "    def get_item(self, index):\r\n",
        "        return self.train_data[index]\r\n",
        "    \r\n",
        "    def get_items_from(self, index):\r\n",
        "        return self.train_data[index : ]\r\n",
        "    \r\n",
        "    def size(self):\r\n",
        "        return len(self.train_data)\r\n",
        "    \r\n",
        "    def is_contain(self, item):\r\n",
        "        for current_item in self.train_data:\r\n",
        "            if current_item.last_item == item : \r\n",
        "                return True\r\n",
        "        return False\r\n",
        "        \r\n",
        "    def sort(self):\r\n",
        "        self.train_data.sort(key=lambda x: x.last_item, reverse=False)\r\n",
        "    \r\n",
        "    def add_item(self, hash_item):\r\n",
        "        self.train_data.append(hash_item)\r\n",
        "        \r\n",
        "    def find_item(self, item):\r\n",
        "        left = 0\r\n",
        "        right = len(self.train_data) - 1\r\n",
        "        while (left <= right):\r\n",
        "            pivot = int((left + right)/2)\r\n",
        "            if self.train_data[pivot].last_item == item: \r\n",
        "                return pivot\r\n",
        "            if self.train_data[pivot].last_item < item:\r\n",
        "                left = pivot + 1\r\n",
        "            else:\r\n",
        "                right = pivot - 1 \r\n",
        "        return -1\r\n",
        "        \r\n",
        "    def add_tid(self, item, tid):\r\n",
        "        index = self.find_item(item)\r\n",
        "        if index == -1:\r\n",
        "            hash_item = HashItem(item)\r\n",
        "            hash_item.add_tid(tid)\r\n",
        "            \r\n",
        "            index = len(self.train_data) - 1\r\n",
        "            self.train_data.append(hash_item)\r\n",
        "            \r\n",
        "            while index >= 0:\r\n",
        "                if self.train_data[index].last_item > item:\r\n",
        "                    self.train_data[index + 1] = self.train_data[index]\r\n",
        "                    index -= 1\r\n",
        "                else:\r\n",
        "                    break\r\n",
        "            self.train_data[index + 1] = hash_item        \r\n",
        "        else:\r\n",
        "            self.train_data[index].add_tid(tid)\r\n",
        "    \r\n",
        "    def serialize(self):\r\n",
        "        temp = []\r\n",
        "        for item in self.train_data:\r\n",
        "            temp.append(item.serialize())\r\n",
        "        return json.dumps(temp)\r\n",
        "\r\n",
        "    def deserialize(self, json_string):\r\n",
        "        self.train_data = []\r\n",
        "        \r\n",
        "        temp = json.loads(json_string)\r\n",
        "        for item_string in temp:\r\n",
        "            item = HashItem(None)\r\n",
        "            item.deserialize(item_string)\r\n",
        "            self.train_data.append(item)\r\n",
        "        "
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5pLFTN5Rfcs"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class ItemsetDictionary(object):\r\n",
        "    \r\n",
        "\r\n",
        "    def __init__(self, ntransactions = 0):\r\n",
        "        self.itemsets = {}\r\n",
        "        self.ntransactions = ntransactions\r\n",
        "            \r\n",
        "    def size(self):\r\n",
        "        return len(self.itemsets)\r\n",
        "    \r\n",
        "    def exists(self, itemset_key):\r\n",
        "        return itemset_key in self.itemsets\r\n",
        "    \r\n",
        "    def add_itemset(self, itemset_key, amount):\r\n",
        "        self.itemsets[itemset_key] = amount\r\n",
        "        \r\n",
        "    def clear(self):\r\n",
        "        self.itemsets.clear()\r\n",
        "    \r\n",
        "    def convert_2_indexes(self):\r\n",
        "        k = 0\r\n",
        "        dict_items_indexes = {}\r\n",
        "        for item_name, _ in self.itemsets.items():\r\n",
        "            dict_items_indexes[item_name] = k\r\n",
        "            k += 1\r\n",
        "        return dict_items_indexes\r\n",
        "            \r\n",
        "    def get_names(self):\r\n",
        "        return self.itemsets.keys()\r\n",
        "        \r\n",
        "    def get_frequency(self, itemset_key):\r\n",
        "        if self.exists(itemset_key):\r\n",
        "            return self.itemsets[itemset_key]\r\n",
        "        return 0\r\n",
        "        \r\n",
        "    def getConfidence(self, rule):\r\n",
        "        left = self.get_frequency(rule.lhs_string())\r\n",
        "        both = self.get_frequency(rule.rule_itemset_2_string())\r\n",
        "        if left == 0: return 0\r\n",
        "        return both/left\r\n",
        "    \r\n",
        "    def get_frequency_combo(self, rule):\r\n",
        "        left = self.get_frequency(rule.lhs_string())\r\n",
        "        right =self.get_frequency(rule.rhs_string())\r\n",
        "        both = self.get_frequency(rule.rule_itemset_2_string())\r\n",
        "        \r\n",
        "        return left, right, both\r\n",
        "    \r\n",
        "    def get_support(self, itemset_key):     \r\n",
        "        return self.get_frequency(itemset_key)/self.ntransactions\r\n",
        "       \r\n",
        "    def split(self, nchunks):\r\n",
        "        itemsets_names = self.itemsets.keys()\r\n",
        "        nitemsets = len(itemsets_names)\r\n",
        "        \r\n",
        "        print ('Number of frequent item-sets: ' + str(nitemsets))\r\n",
        "        itemset_chunks = [[] for _ in range(nchunks)]\r\n",
        "        size_of_chunk = (int)(nitemsets/nchunks) + 1\r\n",
        "                    \r\n",
        "        index = 0\r\n",
        "        counter = 0\r\n",
        "        \r\n",
        "        for itemset_key in itemsets_names:\r\n",
        "            if counter < size_of_chunk:\r\n",
        "                itemset_chunks[index].append(string_2_itemset(itemset_key))\r\n",
        "                counter += 1\r\n",
        "            elif counter == size_of_chunk:\r\n",
        "                index += 1\r\n",
        "                itemset_chunks[index].append(string_2_itemset(itemset_key))\r\n",
        "                counter = 1  \r\n",
        "                  \r\n",
        "        return itemset_chunks\r\n",
        "    \r\n",
        "    def save_2_file(self, file_name, write_mode = 'a', write_support = False):\r\n",
        "        with open(file_name, write_mode) as text_file:\r\n",
        "            for key, value in self.itemsets.items():\r\n",
        "                t = value\r\n",
        "                if write_support == True:\r\n",
        "                    t = value/self.ntransactions\r\n",
        "                text_file.write(key + ':' + str(t))\r\n",
        "                text_file.write('\\n')\r\n",
        "            \r\n",
        "    def load_from_file(self,file_name):\r\n",
        "        self.itemsets.clear()\r\n",
        "        \r\n",
        "        with open(file_name, \"r\") as text_file:\r\n",
        "            self.ntransactions = int(text_file.readline())\r\n",
        "            for line in text_file:\r\n",
        "                #print (line)\r\n",
        "                subStrings = line.split(':')\r\n",
        "                itemset_key = subStrings[0].strip()\r\n",
        "                frequency = int(subStrings[1].strip())\r\n",
        "                \r\n",
        "                self.itemsets[itemset_key] = frequency\r\n",
        "                \r\n",
        "    def _complement_condition(self, r1, r2):\r\n",
        "        merged_itemset = merge_itemsets(r1.left_items, \r\n",
        "                                        r2.left_items)\r\n",
        "        \r\n",
        "        s = self.get_frequency(itemset_2_string(merged_itemset))\r\n",
        "        sl = self.get_frequency(r1.lhs_string())\r\n",
        "        sr = self.get_frequency(r2.lhs_string())\r\n",
        "    \r\n",
        "        #if s > 0: return True\r\n",
        "        return max(s/sl, s/sr)\r\n",
        "     \r\n",
        "        \r\n",
        "    '''\r\n",
        "    Check if two rules are contrary each other based on the matching function\r\n",
        "    r1, r2: dictionaries includes {'r': rule, 'f': feature vector}\r\n",
        "    contrast_params: contains thresholds, and size of LHS, RHS features \r\n",
        "    '''\r\n",
        "    def is_contrast(self, r1, r2, contrast_params):\r\n",
        "        \r\n",
        "        n = contrast_params.n_lhs_features\r\n",
        "        a = cosine_similarity(np.reshape(r1['f'][n:], (1, -1)),\r\n",
        "                              np.reshape(r2['f'][n:], (1, -1)))[0,0]\r\n",
        "        if a > contrast_params.delta2: return (False, 0, 0)\r\n",
        "        \r\n",
        "        b = cosine_similarity(np.reshape(r1['f'][:n], (1, -1)), \r\n",
        "                              np.reshape(r2['f'][:n], (1, -1)))[0,0]\r\n",
        "        if b <= contrast_params.delta1: return (False, 0, 0)\r\n",
        "        \r\n",
        "        t = self._complement_condition(r1['r'], r2['r'])\r\n",
        "        if t > contrast_params.share_threshold:\r\n",
        "            return (True, b, t)\r\n",
        "        return (False, 0, 0)\r\n",
        "    \r\n",
        "    \r\n",
        "    def is_inner_contrast(self, group, contrast_params):\r\n",
        "        #print('check inner')\r\n",
        "        both_condition = self.find_pottential_contrast_locs(group, group, contrast_params)\r\n",
        "        if both_condition is None: return False \r\n",
        "        \r\n",
        "        for i in range(len(both_condition[0])):\r\n",
        "            x = both_condition[0][i]\r\n",
        "            y = both_condition[1][i]\r\n",
        "            if x >= y: continue\r\n",
        "            t = self._complement_condition(group['r'][x], group['r'][y])\r\n",
        "            if t > contrast_params.share_threshold: return True \r\n",
        "            \r\n",
        "        return False\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "    def find_pottential_contrast_locs(self, group1, group2, contrast_params):\r\n",
        "        rhs_sim = cosine_similarity(group1['rhs'], group2['rhs']) \r\n",
        "        rhs_condition = (rhs_sim > contrast_params.delta2).astype(int) \r\n",
        "        if np.all(rhs_condition > 0) == True: return None \r\n",
        "    \r\n",
        "        \r\n",
        "        lhs_sim = cosine_similarity(group1['lhs'], group2['lhs'])\r\n",
        "        lhs_condition = (lhs_sim <= contrast_params.delta1).astype(int)\r\n",
        "        if np.all(lhs_condition > 0) == True: return None \r\n",
        "        \r\n",
        "        locs = np.where(lhs_condition + rhs_condition <= 0)\r\n",
        "        return locs \r\n",
        "        \r\n",
        "    def is_outer_contrast(self, group1, group2, contrast_params):\r\n",
        "        #print('check outer')\r\n",
        "        both_condition = self.find_pottential_contrast_locs(group1, group2, contrast_params)\r\n",
        "        if both_condition is None: return False \r\n",
        "        \r\n",
        "        for i in range(len(both_condition[0])):\r\n",
        "            x = both_condition[0][i]\r\n",
        "            y = both_condition[1][i]\r\n",
        "            t = self._complement_condition(group1['r'][x], group2['r'][y])\r\n",
        "            if t > contrast_params.share_threshold: return True \r\n",
        "            \r\n",
        "        return False\r\n",
        "    "
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwXgNJ16RMPL"
      },
      "source": [
        "class HashTable:\r\n",
        "    def __init__(self):\r\n",
        "        self.table = {}\r\n",
        "        \r\n",
        "    def size(self):\r\n",
        "        return len(self.table)\r\n",
        "    \r\n",
        "    def is_empty(self):\r\n",
        "        return len(self.table) == 0;\r\n",
        "    \r\n",
        "    def is_contain(self, key, last_item):\r\n",
        "        return (key in self.table) and (self.table[key].is_contain(last_item))\r\n",
        "    \r\n",
        "    def get_items(self):\r\n",
        "        return self.table.items()\r\n",
        "    \r\n",
        "    # insert a new key into the table\r\n",
        "    def insert_key(self, key):\r\n",
        "        self.table[key] = HashItemCollection()\r\n",
        "    \r\n",
        "    def insert(self, key, value):\r\n",
        "        self.table[key] = value\r\n",
        "            \r\n",
        "    # remove a key from the table\r\n",
        "    def remove_item(self, key):\r\n",
        "        self.table.pop(key, None)\r\n",
        "        \r\n",
        "    # insert a new transaction id into a specific item-set\r\n",
        "    def add_tid(self, key, item, tid):\r\n",
        "        self.table[key].add_tid(item, tid)\r\n",
        "        \r\n",
        "    # insert a item set and its transaction \r\n",
        "    def add_item(self, key, hash_item):\r\n",
        "        self.table[key].add_item(hash_item)\r\n",
        "    \r\n",
        "    # get all item-set in the hash table\r\n",
        "    def generate_itemset_dictionary(self):\r\n",
        "        collection = ItemsetDictionary()\r\n",
        "        for key, hash_item_collection in self.table.items():\r\n",
        "            for hash_item in hash_item_collection:\r\n",
        "                new_key = ''\r\n",
        "                if key == '': \r\n",
        "                    new_key = hash_item.last_item\r\n",
        "                else:\r\n",
        "                    new_key = key + ',' + hash_item.last_item\r\n",
        "                collection.add_itemset(new_key, hash_item.size())\r\n",
        "        return collection\r\n",
        "    \r\n",
        "    def generate_itemset_dictionary_vw(self, output_file, write_mode):\r\n",
        "        count = 0\r\n",
        "        file_writer = open(output_file, write_mode)\r\n",
        "        for key, hash_item_collection in self.table.items():\r\n",
        "            for hash_item in hash_item_collection:\r\n",
        "                new_key = ''\r\n",
        "                if key == '': \r\n",
        "                    new_key = hash_item.last_item\r\n",
        "                else:\r\n",
        "                    new_key = key + ',' + hash_item.last_item\r\n",
        "                file_writer.write(new_key + ':' + str(hash_item.size()))\r\n",
        "                file_writer.write('\\n')\r\n",
        "                count += 1\r\n",
        "                    \r\n",
        "        file_writer.close()\r\n",
        "        return count\r\n",
        "    \r\n",
        "    # get number of item-set have same K - 1 first items.\r\n",
        "    def count_itemsets(self, key):\r\n",
        "        return self.table[key].size()\r\n",
        "    \r\n",
        "    # get frequent item-set\r\n",
        "    def generate_frequent_itemsets(self, minsup):\r\n",
        "        L = HashTable()\r\n",
        "        for key, hash_item_collection in self.table.items():\r\n",
        "            L.insert_key(key)\r\n",
        "            for hash_item in hash_item_collection:\r\n",
        "                if hash_item.size() >= minsup:\r\n",
        "                    L.add_item(key, hash_item)\r\n",
        "            if L.count_itemsets(key) == 0:\r\n",
        "                L.remove_item(key)\r\n",
        "        return L\r\n",
        "                 \r\n",
        "    def sort(self):\r\n",
        "        for hash_item_collection in self.table.values():\r\n",
        "            hash_item_collection.sort()\r\n",
        "\r\n",
        "    # this function is used for multi-thread\r\n",
        "    def append(self, other_hash_table):\r\n",
        "   \r\n",
        "        for key, hash_item_collection in other_hash_table.get_items():\r\n",
        "            self.table[key] = hash_item_collection\r\n",
        "\r\n",
        "    def clear(self):\r\n",
        "        self.table.clear()\r\n",
        "        \r\n",
        "    def split(self, n):\r\n",
        "        number_of_keys = self.size()\r\n",
        "        if number_of_keys < n:\r\n",
        "            return [self]\r\n",
        "        \r\n",
        "        number_for_each_part = (int)(number_of_keys/n) + 1\r\n",
        "        counter = 0\r\n",
        "        sub_hash_tables = []\r\n",
        "        sub_hash_table = HashTable()\r\n",
        "        \r\n",
        "        for key, hash_item_collection in self.get_items():\r\n",
        "            if counter < number_for_each_part:\r\n",
        "                sub_hash_table.insert(key, hash_item_collection)\r\n",
        "            elif counter == number_for_each_part:\r\n",
        "                sub_hash_tables.append(sub_hash_table)\r\n",
        "                sub_hash_table = HashTable()\r\n",
        "                sub_hash_table.insert(key, hash_item_collection)\r\n",
        "                counter = 0\r\n",
        "            counter += 1\r\n",
        "        sub_hash_tables.append(sub_hash_table)\r\n",
        "        return sub_hash_tables     \r\n",
        "    \r\n",
        "    def serialize(self, file_name):\r\n",
        "        with open(file_name, \"w\") as text_file:\r\n",
        "            #json.dump(self.table, text_file)\r\n",
        "            k = 0\r\n",
        "            for key, value in self.table.items():\r\n",
        "                if k > 0:\r\n",
        "                    text_file.write('\\n')\r\n",
        "                text_file.write(key)\r\n",
        "                text_file.write('\\n')\r\n",
        "                text_file.write(value.serialize())\r\n",
        "                k += 1\r\n",
        "            \r\n",
        "    def deserialize(self, file_name, reset_table = True):\r\n",
        "        if reset_table == True:\r\n",
        "            self.table = {}\r\n",
        "        with open(file_name, \"r\") as text_file:\r\n",
        "            k = 0\r\n",
        "            collection_key = None\r\n",
        "            for line in text_file:\r\n",
        "                if k % 2 == 0:\r\n",
        "                    collection_key = line.strip()\r\n",
        "                else:\r\n",
        "                    collection = HashItemCollection()\r\n",
        "                    collection.deserialize(line.strip())\r\n",
        "                    self.table[collection_key] = collection\r\n",
        "                k = k + 1"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRLflKC3QUBU"
      },
      "source": [
        "from multiprocessing import Process\r\n",
        "from multiprocessing.managers import BaseManager\r\n",
        "\r\n",
        "class Apriori:\r\n",
        "    def __init__(self, train_data_set):\r\n",
        "        self.tmp_folder = '\\\\Content\\\\'\r\n",
        "        self.freq_itemsets_tmp_file = self.tmp_folder + 'freqitemsets.tmp'\r\n",
        "        self.itemsets_tmp_file = self.tmp_folder + 'itemsetscandidates.tmp'\r\n",
        "        self.freq_k_item_sets_tmp_file = self.tmp_folder + 'freq_k_itemsets.tmp'\r\n",
        "        self.data_set = train_data_set\r\n",
        "        self.L1 = None\r\n",
        "        \r\n",
        "       \r\n",
        "        \r\n",
        "    \r\n",
        "    def generate_L1(self, min_sup):\r\n",
        "        C_1 = HashTable()\r\n",
        "        itemset_key = ''\r\n",
        "        C_1.insert_key(itemset_key)\r\n",
        "    \r\n",
        "        n = self.data_set.size()\r\n",
        "        print ('size of data-set: ' + str(n))\r\n",
        "        \r\n",
        "        for tid in range(n):\r\n",
        "            transaction = self.data_set.get_transaction(tid)\r\n",
        "            for item in transaction:\r\n",
        "                C_1.add_tid(itemset_key, item, tid)\r\n",
        "            \r\n",
        "        print ('get frequent item sets with 1 item')\r\n",
        "        self.L1 = C_1.generate_frequent_itemsets(min_sup)\r\n",
        "      \r\n",
        "    @staticmethod\r\n",
        "    def generate_Lk(min_sup, L_k1, C_k, k):\r\n",
        "        print('generate candidates with ' + str(k) + ' items')\r\n",
        "        for key, hash_item_collection in L_k1.get_items():\r\n",
        "            for index in range(hash_item_collection.size() - 1):\r\n",
        "                \r\n",
        "                index_th_item = hash_item_collection.get_item(index)\r\n",
        "                new_key = ''\r\n",
        "                if key == '':\r\n",
        "                    new_key = index_th_item.last_item\r\n",
        "                else:\r\n",
        "                    new_key = key +',' + index_th_item.last_item\r\n",
        "                new_hash_collection = HashItemCollection()\r\n",
        "                \r\n",
        "                #check if it is infrequent item-set\r\n",
        "                for item in hash_item_collection.get_items_from(index + 1):\r\n",
        "                    new_item = HashItem(item.last_item)\r\n",
        "                    inter_items = set(index_th_item.tids).intersection(item.tids)      \r\n",
        "                    if len(inter_items) >= min_sup:  \r\n",
        "                        new_item.add_tids(list(inter_items))\r\n",
        "                        new_hash_collection.add_item(new_item)\r\n",
        "                        \r\n",
        "                if new_hash_collection.size() > 0:        \r\n",
        "                    C_k.insert(new_key,  new_hash_collection) \r\n",
        "\r\n",
        "    def generate_frequent_itemsets(self, min_sup, nthreads, end, output_file, write_support = False):\r\n",
        "        \r\n",
        "        '''\r\n",
        "        Step 1: Generate frequent item-sets with 1 item and write to file\r\n",
        "        '''\r\n",
        "        nTransactions = self.data_set.size()\r\n",
        "        with open(output_file, 'w') as text_file:\r\n",
        "            text_file.write(str(nTransactions))\r\n",
        "            text_file.write('\\n')\r\n",
        "        \r\n",
        "        \r\n",
        "        self.generate_L1(min_sup)\r\n",
        "        freq_itemsets_dict = self.L1.generate_itemset_dictionary()\r\n",
        "        freq_itemsets_dict.ntransactions = nTransactions\r\n",
        "        freq_itemsets_dict.save_2_file(output_file, 'a', write_support)\r\n",
        "        freq_itemsets_dict.clear()\r\n",
        "        \r\n",
        "        '''\r\n",
        "        Step 2: Generate frequent item-sets with more than 1 item and append to the file\r\n",
        "        '''\r\n",
        "        k = 2    \r\n",
        "        L_k1 = self.L1\r\n",
        "        \r\n",
        "        while not L_k1.is_empty() and (end == -1 or k <= end):\r\n",
        "            \r\n",
        "            print('extracting item-sets with ' + str(k) + ' items ....')\r\n",
        "            \r\n",
        "            '''\r\n",
        "            Divide data into many parts and create processes to generate frequent item-sets\r\n",
        "            '''\r\n",
        "            L_k = HashTable()\r\n",
        "            chunks = L_k1.split(nthreads)\r\n",
        "            processes = []\r\n",
        "            \r\n",
        "            C_ks = []\r\n",
        "            BaseManager.register(\"AprioriHash\", HashTable)\r\n",
        "            manager = BaseManager()\r\n",
        "            manager.start()\r\n",
        "            C_ks.append(manager.AprioriHash())\r\n",
        "            \r\n",
        "            index = 0\r\n",
        "            for L_k_1_chunk in chunks:\r\n",
        "                process_i = Process(target = Apriori.generate_Lk, \r\n",
        "                                    args=(min_sup, L_k_1_chunk,C_ks[index], k))\r\n",
        "                processes.append(process_i)\r\n",
        "                index += 1\r\n",
        "            \r\n",
        "            # wait for all thread completes\r\n",
        "            for process_i in processes:\r\n",
        "                process_i.start()\r\n",
        "                process_i.join()\r\n",
        "             \r\n",
        "            '''\r\n",
        "            Merge results which returns from processes\r\n",
        "            '''\r\n",
        "            for new_C_k in C_ks:\r\n",
        "                L_k.append(new_C_k)\r\n",
        "            L_k1.clear()\r\n",
        "            L_k1 = L_k\r\n",
        "    \r\n",
        "            '''\r\n",
        "            Append frequent item-sets with k items to file\r\n",
        "            '''\r\n",
        "            freq_itemsets_dict = L_k1.generate_itemset_dictionary()\r\n",
        "            \r\n",
        "            print ('Writing frequent itemset to file ' + str(freq_itemsets_dict.size()))\r\n",
        "            freq_itemsets_dict.ntransactions = nTransactions\r\n",
        "            freq_itemsets_dict.save_2_file(output_file, 'a', write_support)\r\n",
        "            freq_itemsets_dict.clear()\r\n",
        "            \r\n",
        "            k += 1\r\n",
        "            \r\n",
        "        print ('stop at k = ' + str(k))\r\n",
        "     \r\n",
        "    @staticmethod\r\n",
        "    def generate_Lk_vw(min_sup, L_k1, C_k_file, k):\r\n",
        "        print('generate candidates with ' + str(k) + ' items')\r\n",
        "        file_writer = open(C_k_file, 'w') \r\n",
        "        for key, hash_item_collection in L_k1.get_items():\r\n",
        "            for index in range(hash_item_collection.size() - 1):\r\n",
        "                \r\n",
        "                index_th_item = hash_item_collection.get_item(index)\r\n",
        "                new_key = ''\r\n",
        "                if key == '':\r\n",
        "                    new_key = index_th_item.last_item\r\n",
        "                else:\r\n",
        "                    new_key = key +',' + index_th_item.last_item\r\n",
        "                new_hash_collection = HashItemCollection()\r\n",
        "                \r\n",
        "                #check if it is infrequent item-set\r\n",
        "                for item in hash_item_collection.get_items_from(index + 1):\r\n",
        "                    new_item = HashItem(item.last_item)\r\n",
        "                    inter_items = set(index_th_item.tids).intersection(item.tids)      \r\n",
        "                    if len(inter_items) >= min_sup:  \r\n",
        "                        new_item.add_tids(list(inter_items))\r\n",
        "                        new_hash_collection.add_item(new_item)\r\n",
        "                        \r\n",
        "                if new_hash_collection.size() > 0:  \r\n",
        "                    file_writer.write(new_key)\r\n",
        "                    file_writer.write('\\n')\r\n",
        "                    file_writer.write(new_hash_collection.serialize())      \r\n",
        "                    file_writer.write('\\n')\r\n",
        "        file_writer.close()\r\n",
        "\r\n",
        "    def generate_frequent_itemsets_vw(self, min_sup, nThreads, end, output_file):\r\n",
        "        \r\n",
        "        '''\r\n",
        "        Step 1: Generate frequent item-sets with 1 item and write to file\r\n",
        "        '''\r\n",
        "        ntransactions = self.data_set.size()\r\n",
        "        with open(output_file, 'w') as text_file:\r\n",
        "            text_file.write(str(ntransactions))\r\n",
        "            text_file.write('\\n')\r\n",
        "        \r\n",
        "        \r\n",
        "        self.generate_L1(min_sup)\r\n",
        "        self.L1.generate_itemset_dictionary_vw(output_file, 'a')\r\n",
        "        \r\n",
        "        '''\r\n",
        "        Step 2: Generate frequent item-sets with more than 1 item and append to the file\r\n",
        "        '''\r\n",
        "        k = 2    \r\n",
        "        L_k1 = self.L1\r\n",
        "        \r\n",
        "        while not L_k1.is_empty() and (end == -1 or k <= end):\r\n",
        "            \r\n",
        "            print('extracting item-sets with ' + str(k) + ' items ....')\r\n",
        "            \r\n",
        "            '''\r\n",
        "            Divide data into many parts and create processes to generate frequent item-sets\r\n",
        "            '''\r\n",
        "            chunks = L_k1.split(nThreads)\r\n",
        "            L_k1 = None\r\n",
        "            processes = []\r\n",
        "            \r\n",
        "            index = 0\r\n",
        "            for L_k_1_chunk in chunks:\r\n",
        "                chunk_output_file = self.freq_itemsets_tmp_file +'.'+ str(index)\r\n",
        "                process_i = Process(target = Apriori.generate_Lk_vw, \r\n",
        "                                    args=(min_sup, L_k_1_chunk,chunk_output_file, k))\r\n",
        "                processes.append(process_i)\r\n",
        "                index += 1\r\n",
        "            \r\n",
        "            # wait for all thread completes\r\n",
        "            for process_i in processes:\r\n",
        "                process_i.start()\r\n",
        "                process_i.join()\r\n",
        "             \r\n",
        "            '''\r\n",
        "            Merge results which returns from processes\r\n",
        "            '''\r\n",
        "            L_k1 = HashTable()\r\n",
        "            for index in range(len(chunks)):\r\n",
        "                chunk_input_file = self.freq_itemsets_tmp_file +'.'+ str(index)\r\n",
        "                L_k1.deserialize(chunk_input_file, False)\r\n",
        "            \r\n",
        "            '''\r\n",
        "            Append frequent item-sets with k items to file\r\n",
        "            '''\r\n",
        "            print ('Writing frequent itemset to file....')\r\n",
        "            x = L_k1.generate_itemset_dictionary_vw(output_file, 'a')\r\n",
        "            print ('#item-sets: ' + str(x))\r\n",
        "            k += 1\r\n",
        "            \r\n",
        "        print ('stop at k = ' + str(k))\r\n",
        "\r\n",
        "    def get_item_interaction_matrix(self):\r\n",
        "        self.generate_L1(0)\r\n",
        "        items_dict = self.L1.generate_itemset_dictionary()\r\n",
        "        items_dict.nTransaction = self.data_set.size()\r\n",
        "        \r\n",
        "        nItems = items_dict.size()\r\n",
        "        dict_item_indexes = items_dict.convert_2_indexes()\r\n",
        "            \r\n",
        "        A = np.zeros((nItems, nItems))\r\n",
        "        for transaction in self.data_set:\r\n",
        "            indexes = []\r\n",
        "            for item_name in transaction:\r\n",
        "                indexes.append(dict_item_indexes[item_name])\r\n",
        "            for i in range(len(indexes)):\r\n",
        "                for j in range(i+1, len(indexes)):\r\n",
        "                    A[indexes[i], indexes[j]] += 1\r\n",
        "                    A[indexes[j], indexes[i]] += 1\r\n",
        "        return dict_item_indexes, A\r\n",
        "    \r\n",
        "        \r\n",
        "        "
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fl1AeTiYr85"
      },
      "source": [
        "class ItemsetFormatter(object):\r\n",
        "   \r\n",
        "    @staticmethod\r\n",
        "    def mydefault(itemset):\r\n",
        "        return True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def mass(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if item.isdigit() == False:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcr(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if item == 'CD4' or item == 'CD8':\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def rna(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if 'rna_' in item:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def ank3(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if item == 'CASE' or item == 'HEALTHY':\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def spect(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if 'class@' in item:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def kdd(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if 'c_' in item:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcrm(itemset):\r\n",
        "        a_count = 0\r\n",
        "        b_count = 0\r\n",
        "        for item in itemset:\r\n",
        "            if 'b_' in item:\r\n",
        "                b_count += 1\r\n",
        "            if 'a_' in item:\r\n",
        "                a_count += 1\r\n",
        "        return (a_count > 0 and b_count > 0)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def ppi(itemset):\r\n",
        "        a_count = 0\r\n",
        "        b_count = 0\r\n",
        "        for item in itemset:\r\n",
        "            if 'h@' in item:\r\n",
        "                b_count += 1\r\n",
        "            if 'v@' in item:\r\n",
        "                a_count += 1\r\n",
        "        return (a_count > 0 and b_count > 0)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def splice(itemset):\r\n",
        "        for item in itemset:\r\n",
        "            if item == 'EI' or item == 'IE' or item == 'N@':\r\n",
        "                return True\r\n",
        "        return False"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fehf8m7bY5Zh"
      },
      "source": [
        "class RuleFormatter(object):\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def mydefaultLeft(item):\r\n",
        "        return True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def mydefaultRight(item):\r\n",
        "        return True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def mydefault(rule):\r\n",
        "        #return True\r\n",
        "        return len(rule.right_items) == 1#<= 2\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def massLeft(item):\r\n",
        "        return item.isdigit()\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def massRight(item):\r\n",
        "        return not item.isdigit()\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def mass(rule):\r\n",
        "        return rule.lhs_string().isdigit() and (not rule.rhs_string().isdigit())\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def rna(rule):\r\n",
        "        condition = (len(rule.right_items) == 1)\r\n",
        "        condition &= ('rna_' in rule.rhs_string())\r\n",
        "        condition &=  ('rna_' not in rule.lhs_string())\r\n",
        "        return condition\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcrLeft(item):\r\n",
        "        return item != 'CD4' and item != 'CD8'\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcrRight(item):\r\n",
        "        return item == 'CD4' or item == 'CD8'    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcr(rule):\r\n",
        "        left_key = rule.lhs_string()\r\n",
        "        right_key = rule.rhs_string()\r\n",
        "        return ('CD4' not in left_key) and ('CD8' not in left_key) and (right_key == 'CD4' or right_key == 'CD8')\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def ank3Left(item):\r\n",
        "        return item != 'CASE' and item != 'HEALTHY'\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def ank3Right(item):\r\n",
        "        return item == 'CASE' or item == 'HEALTHY'\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def ank3(rule):\r\n",
        "        left_key = rule.lhs_string()\r\n",
        "        right_key = rule.rhs_string()\r\n",
        "        return ('CASE' not in left_key) and ('HEALTHY' not in left_key) and (right_key == 'CASE' or right_key == 'HEALTHY')\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def spectLeft(item):\r\n",
        "        return 'class@' not in item\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def spectRight(item):\r\n",
        "        return 'class@' in item\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def spect(rule):\r\n",
        "        flag = True\r\n",
        "        for item in rule.right_items:\r\n",
        "            if 'class@' not in item:\r\n",
        "                flag = False\r\n",
        "                break\r\n",
        "        left_key = rule.lhs_string()\r\n",
        "        return ('class@' not in left_key) and flag == True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def kddLeft(item):\r\n",
        "        return ('c_' in item) == False\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def kddRight(item):\r\n",
        "        return 'c_' in item\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def kdd(rule):\r\n",
        "        left_key = rule.lhs_string()\r\n",
        "        right_key = rule.rhs_string()\r\n",
        "        return ('c_' not in left_key) and (len(rule.right_items) == 1 and 'c_' in right_key)\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcrmLeft(item):\r\n",
        "        return True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def tcrmRight(item):\r\n",
        "        return True\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def tcrm(rule):\r\n",
        "        a_count1 = 0\r\n",
        "        b_count1 = 0\r\n",
        "        for item in rule.left_items:\r\n",
        "            if 'b_' in item:\r\n",
        "                b_count1 += 1\r\n",
        "            if 'a_' in item:\r\n",
        "                a_count1 += 1\r\n",
        "        if a_count1 > 0 and b_count1 > 0: return False\r\n",
        "        \r\n",
        "        a_count2 = 0\r\n",
        "        b_count2 = 0\r\n",
        "        for item in rule.right_items:\r\n",
        "            if 'b_' in item:\r\n",
        "                b_count2 += 1\r\n",
        "            if 'a_' in item:\r\n",
        "                a_count2 += 1\r\n",
        "        if a_count2 > 0 and b_count2 > 0: return False\r\n",
        "        \r\n",
        "        return (a_count1 > 0 and b_count2 > 0) or (b_count1 > 0 and a_count2 > 0)\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def ppiLeft(item):\r\n",
        "        return True\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def ppiRight(item):\r\n",
        "        return True\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def ppi(rule):\r\n",
        "        a_count1 = 0\r\n",
        "        b_count1 = 0\r\n",
        "        for item in rule.left_items:\r\n",
        "            if 'h@' in item:\r\n",
        "                b_count1 += 1\r\n",
        "            if 'v@' in item:\r\n",
        "                a_count1 += 1\r\n",
        "        if a_count1 > 0 and b_count1 > 0: return False\r\n",
        "        \r\n",
        "        a_count2 = 0\r\n",
        "        b_count2 = 0\r\n",
        "        for item in rule.right_items:\r\n",
        "            if 'h@' in item:\r\n",
        "                b_count2 += 1\r\n",
        "            if 'v@' in item:\r\n",
        "                a_count2 += 1\r\n",
        "        if a_count2 > 0 and b_count2 > 0: return False\r\n",
        "        \r\n",
        "        return (a_count1 > 0 and b_count2 > 0) or (b_count1 > 0 and a_count2 > 0)\r\n",
        "    \r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def spliceLeft(item):\r\n",
        "        return item != 'EI' and item != 'IE' and item != 'N@'\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def spliceRight(item):\r\n",
        "        return item == 'D_0' or item == 'D_1' or item == 'N@'\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def splice(rule):\r\n",
        "        left_key = rule.lhs_string()\r\n",
        "        right_key = rule.rhs_string()\r\n",
        "        return ('EI' not in left_key) and ('IE' not in left_key) and ('N@' not in left_key) and (right_key == 'EI' or right_key == 'IE' or right_key == 'N@')\r\n",
        "        "
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfV8gDvib0Kw"
      },
      "source": [
        "class AssociationRule:\r\n",
        "    def __init__(self, left, right):\r\n",
        "        self.left_items = left\r\n",
        "        self.right_items = right\r\n",
        "        self.scores = []\r\n",
        "        \r\n",
        "    def length(self):\r\n",
        "        return len(self.left_items) + len(self.right_items)\r\n",
        "     \r\n",
        "    def score(self, index):\r\n",
        "        return self.scores[index]\r\n",
        "    \r\n",
        "    def lhs_string(self):\r\n",
        "        return itemset_2_string(self.left_items)\r\n",
        "        \r\n",
        "    def rhs_string(self):\r\n",
        "        return itemset_2_string(self.right_items)\r\n",
        "    \r\n",
        "    def serialize(self):\r\n",
        "        left_key = self.lhs_string()\r\n",
        "        right_key = self.rhs_string()\r\n",
        "        return left_key + \">\" + right_key\r\n",
        "    \r\n",
        "    @staticmethod        \r\n",
        "    def string_2_rule(s):\r\n",
        "        subStrings = s.split(\">\")\r\n",
        "        left = string_2_itemset(subStrings[0].strip())\r\n",
        "        right = string_2_itemset(subStrings[1].strip())\r\n",
        "        #print(\"AssociationRule(left, right\",AssociationRule(left, right))\r\n",
        "        return AssociationRule(left, right)\r\n",
        "\r\n",
        "    def append_score(self, score):\r\n",
        "        self.scores.append(score)\r\n",
        "        \r\n",
        "    def get_itemset(self):\r\n",
        "        itemset = []\r\n",
        "        itemset.extend(self.left_items)\r\n",
        "        itemset.extend(self.right_items)\r\n",
        "        itemset.sort()\r\n",
        "        return itemset\r\n",
        "        \r\n",
        "        \r\n",
        "    def rule_itemset_2_string(self):\r\n",
        "        itemset = self.get_itemset()\r\n",
        "        return itemset_2_string(itemset)\r\n",
        "    \r\n",
        "    def compute_basic_probs(self,frequent_itemsets, nTransactions):  \r\n",
        "        \r\n",
        "        left = frequent_itemsets[self.lhs_string()]\r\n",
        "        right = frequent_itemsets[self.rhs_string()]\r\n",
        "        \r\n",
        "        both = frequent_itemsets[self.rule_itemset_2_string()]\r\n",
        "        \r\n",
        "        vector = {}\r\n",
        "        \r\n",
        "        ''' 1. P(A)'''\r\n",
        "        p_A = left/nTransactions\r\n",
        "        vector['A'] = p_A\r\n",
        "        \r\n",
        "        ''' 2. P(B)'''\r\n",
        "        p_B = right/nTransactions\r\n",
        "        vector['B'] = p_B\r\n",
        "        \r\n",
        "        ''' 3. P(~A)'''\r\n",
        "        p_not_A = 1 - p_A\r\n",
        "        vector['~A'] = p_not_A\r\n",
        "        \r\n",
        "        ''' 4. P(~B)'''\r\n",
        "        p_not_B = 1 - p_B\r\n",
        "        vector['~B'] = p_not_B\r\n",
        "        \r\n",
        "        ''' 5. P(AB) '''\r\n",
        "        p_A_and_B = both/nTransactions\r\n",
        "        vector['AB'] = p_A_and_B\r\n",
        "        \r\n",
        "        ''' 6. P(~AB)'''\r\n",
        "        p_not_A_and_B = (right - both)/nTransactions\r\n",
        "        vector['~AB'] = p_not_A_and_B\r\n",
        "        \r\n",
        "        ''' 7. P(A~B)'''\r\n",
        "        p_A_and_not_B = (left - both)/nTransactions\r\n",
        "        vector['A~B'] = p_A_and_not_B\r\n",
        "        \r\n",
        "        ''' 8. P(~A~B)'''\r\n",
        "        p_not_A_and_not_B = 1 - (left + right - both)/nTransactions\r\n",
        "        vector['~A~B'] = p_not_A_and_not_B \r\n",
        "        \r\n",
        "        '''\r\n",
        "        9. P(A|B)\r\n",
        "        '''\r\n",
        "        p_A_if_B = p_A_and_B / p_B\r\n",
        "        vector['A|B'] = p_A_if_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        10. P(~A|~B)\r\n",
        "        '''\r\n",
        "        p_not_A_if_not_B = p_not_A_and_not_B / p_not_B\r\n",
        "        vector['~A|~B'] = p_not_A_if_not_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        11. P(A|~B)\r\n",
        "        '''\r\n",
        "        p_A_if_not_B = p_A_and_not_B/p_not_B\r\n",
        "        vector['A|~B'] = p_A_if_not_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        12. p(~A|B)\r\n",
        "        '''\r\n",
        "        p_not_A_if_B = p_not_A_and_B / p_B\r\n",
        "        vector['~A|B'] = p_not_A_if_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        13. P(B|A)\r\n",
        "        '''\r\n",
        "        p_B_if_A = p_A_and_B / p_A\r\n",
        "        vector['B|A'] = p_B_if_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        14. P(~B|~A)\r\n",
        "        '''\r\n",
        "        p_not_B_if_not_A = p_not_A_and_not_B / p_not_A\r\n",
        "        vector['~B|~A'] = p_not_B_if_not_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        15. P(B|~A)\r\n",
        "        '''\r\n",
        "        p_B_if_not_A = p_not_A_and_B/p_not_A\r\n",
        "        vector['B|~A'] = p_B_if_not_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        16. p(~B|A)\r\n",
        "        '''\r\n",
        "        p_not_B_if_A = p_A_and_not_B / p_A\r\n",
        "        vector['~B|A'] = p_not_B_if_A\r\n",
        "        \r\n",
        "        return vector\r\n",
        "    \r\n",
        "    def is_redundant_(self, bits, k, itemset, freq_itemset_dict): \r\n",
        "        '''\r\n",
        "        Run out of items --> create rule and check format criterion\r\n",
        "        '''\r\n",
        "        if k >= len(itemset):\r\n",
        "            items_1 = []\r\n",
        "            items_2 = []\r\n",
        "            for index in range(len(bits)):\r\n",
        "                if bits[index] == True:\r\n",
        "                    items_1.append(itemset[index])\r\n",
        "                else:\r\n",
        "                    items_2.append(itemset[index])\r\n",
        "            for item in items_2:\r\n",
        "                rule = AssociationRule(items_1, [item])\r\n",
        "                confidence = freq_itemset_dict.getConfidence(rule)\r\n",
        "                if confidence == 1: return True\r\n",
        "            return False \r\n",
        "      \r\n",
        "        value_domain = [True, False]\r\n",
        "        for value in value_domain:\r\n",
        "            bits[k] = value\r\n",
        "            checker = self.is_redundant_(bits, k+1, itemset, freq_itemset_dict)\r\n",
        "            if checker == True: return True\r\n",
        "            bits[k] = True    \r\n",
        "        return False\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Expand an item-set with equivalent items.\r\n",
        "    '''\r\n",
        "    def is_redundant(self, freq_itemset_dict):\r\n",
        "        bits = [True for _ in self.left_items]\r\n",
        "        checker = self.is_redundant_(bits, 0, self.left_items, freq_itemset_dict)\r\n",
        "        if checker == True: return True\r\n",
        "        \r\n",
        "        bits =  [True for _ in self.right_items]\r\n",
        "        return self.is_redundant_(bits, 0, self.right_items, freq_itemset_dict)\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Check if an item-set is satisfied condition of the rule. \r\n",
        "    '''\r\n",
        "    def satisfy_rule(self, itemset, is_lhs = True):\r\n",
        "        condition = self.left_items\r\n",
        "        if is_lhs == False: condition = self.right_items\r\n",
        "        if len(condition) > len(itemset) or len(itemset) == 0:\r\n",
        "            return False\r\n",
        "        for item in condition:\r\n",
        "            if item not in itemset:\r\n",
        "                return False\r\n",
        "        return True\r\n",
        "    "
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzL6QVz9biCo"
      },
      "source": [
        "class RulesCollection(object):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.rules = []\r\n",
        "        \r\n",
        "        \r\n",
        "    def size(self):\r\n",
        "        return len(self.rules)\r\n",
        "        \r\n",
        "    def add(self, r):\r\n",
        "        self.rules.append(r)\r\n",
        "        \r\n",
        "    def clear(self):\r\n",
        "        self.rules.clear()\r\n",
        "        \r\n",
        "    def save(self, file_name, is_append):\r\n",
        "        mode = 'w'\r\n",
        "        if is_append == True:\r\n",
        "            mode = 'a'\r\n",
        "        with open(file_name, mode) as text_file:\r\n",
        "            for rule in self.rules:\r\n",
        "                text_file.write(rule.serialize())\r\n",
        "                text_file.write('\\n')\r\n",
        "                \r\n",
        "    def load_from_file(self, file_name):    \r\n",
        "        with open(file_name, \"r\") as text_file:\r\n",
        "            for line in text_file:\r\n",
        "                rule = AssociationRule.string_2_rule(line)\r\n",
        "                self.rules.append(rule)\r\n",
        "        \r\n",
        "    def remove_redundancy(self, freq_itemset_dict):\r\n",
        "        new_rules = []\r\n",
        "        for r in self.rules:\r\n",
        "            if r.is_redundant(freq_itemset_dict):\r\n",
        "                continue\r\n",
        "            new_rules.append(r)\r\n",
        "        self.rules = new_rules \r\n",
        "                \r\n",
        "class RulesDictionary():\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self.rules = {}\r\n",
        "                    \r\n",
        "    def load_from_file(self, file_name):\r\n",
        "        with open(file_name, \"r\") as text_file:\r\n",
        "            for line in text_file:\r\n",
        "                rule = AssociationRule.string_2_rule(line)\r\n",
        "                self.rules[line.strip()] = rule\r\n",
        "    \r\n",
        "    def get_rules(self):\r\n",
        "        return list(self.rules.values())\r\n",
        "    \r\n",
        "    def rule_2_string(self):\r\n",
        "        return list(self.rules.keys())\r\n",
        "    \r\n",
        "    def clear(self):\r\n",
        "        self.rules.clear()"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKNxYr_fZPGJ"
      },
      "source": [
        "class Generator:\r\n",
        "    \r\n",
        "    def __init__(self, freq_itemset_dict, \r\n",
        "                 min_conf, \r\n",
        "                 itemset_formatter, \r\n",
        "                 rule_formatter, \r\n",
        "                 nThreads):\r\n",
        "        self.itemset_formatter = itemset_formatter\r\n",
        "        self.rule_formatter = rule_formatter\r\n",
        "        \r\n",
        "        self.nthreads = nThreads\r\n",
        "        self.freq_itemset_dict = freq_itemset_dict\r\n",
        "        \r\n",
        "        self.min_conf = min_conf\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def string_2_rule_and_support(s):\r\n",
        "        subStrings = s.split('#')\r\n",
        "        rule  = Generator.string_2_rule(subStrings[0].strip())\r\n",
        "        v = json.loads(subStrings[1].strip())\r\n",
        "        return rule, v\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def rule_and_support_2_string(rule, p):\r\n",
        "        return rule.serialize() + '#' + json.dumps(p)\r\n",
        "                \r\n",
        "    '''\r\n",
        "    Generate association rules for one item-set\r\n",
        "    '''\r\n",
        "    def subsets(self, bits, item_set, k, rule_collection, total_freq): \r\n",
        "        '''\r\n",
        "        Run out of items --> create rule and check format criterion\r\n",
        "        '''\r\n",
        "        if k >= len(item_set):\r\n",
        "            left = []\r\n",
        "            right = []\r\n",
        "                    \r\n",
        "            for index in range(len(bits)):\r\n",
        "                if bits[index] == True:\r\n",
        "                    left.append(item_set[index])\r\n",
        "                else:\r\n",
        "                    right.append(item_set[index])\r\n",
        "                                      \r\n",
        "            if (len(left) > 0 and len(right) > 0):\r\n",
        "                rule = AssociationRule(left, right)\r\n",
        "                if (self.rule_formatter == None or self.rule_formatter(rule) == True):\r\n",
        "                    rule_collection.add(rule)\r\n",
        "            \r\n",
        "            return \r\n",
        "      \r\n",
        "        value_domain = [True, False]\r\n",
        "        '''\r\n",
        "        Include k-th item into LHS \r\n",
        "        '''\r\n",
        "        \r\n",
        "        for value in value_domain:\r\n",
        "            bits[k] = value\r\n",
        "               \r\n",
        "            if (value == False):\r\n",
        "                left_itemset = []\r\n",
        "                for index in range(len(bits)):\r\n",
        "                    if bits[index] == True:\r\n",
        "                        left_itemset.append(item_set[index])\r\n",
        "                        \r\n",
        "                left_value = self.freq_itemset_dict.get_frequency(itemset_2_string(left_itemset))\r\n",
        "                confident = 0\r\n",
        "                if left_value > 0: confident = total_freq/left_value\r\n",
        "                \r\n",
        "                if confident < self.min_conf:\r\n",
        "                    bits[k] = True\r\n",
        "                    continue\r\n",
        "                self.subsets(bits, item_set, k+1, rule_collection, total_freq)\r\n",
        "            else:\r\n",
        "                self.subsets(bits, item_set, k+1, rule_collection, total_freq)\r\n",
        "                \r\n",
        "            bits[k] = True\r\n",
        "    '''\r\n",
        "    Generate association rules for a set of item-sets and write results to a file\r\n",
        "    '''\r\n",
        "    def generate_rules(self, freq_itemsets_collection, output_file_name):\r\n",
        "        total_rules = 0\r\n",
        "        remaining_rules = 0\r\n",
        "        k = 0\r\n",
        "        rule_collection = RulesCollection()\r\n",
        "        with open(output_file_name, 'w') as _:\r\n",
        "            print ('clear old file...')\r\n",
        "            \r\n",
        "        for itemset in freq_itemsets_collection:\r\n",
        "            '''\r\n",
        "            Check item-set first if it can generate a rule\r\n",
        "            '''\r\n",
        "            if len(itemset) == 1:\r\n",
        "                continue\r\n",
        "     \r\n",
        "         \r\n",
        "            if self.itemset_formatter is not None and \\\r\n",
        "            self.itemset_formatter(itemset) == False:\r\n",
        "                continue\r\n",
        "            \r\n",
        "            '''\r\n",
        "            Write generated rule_collection into file\r\n",
        "            '''\r\n",
        "            k += 1\r\n",
        "            if k % 200 == 0:\r\n",
        "                print ('writing some rule_collection to file: ' + str(k))\r\n",
        "                total_rules += rule_collection.size()\r\n",
        "                rule_collection.remove_redundancy(self.freq_itemset_dict)\r\n",
        "                rule_collection.save(output_file_name, True)\r\n",
        "                remaining_rules += rule_collection.size()\r\n",
        "                rule_collection.clear()\r\n",
        "            \r\n",
        "            '''\r\n",
        "            Generating association rule_collection.\r\n",
        "            '''\r\n",
        "            total_freq = self.freq_itemset_dict.get_frequency(itemset_2_string(itemset))\r\n",
        "            bits = [True] * len(itemset)\r\n",
        "            self.subsets(bits, itemset, 0, rule_collection, total_freq)\r\n",
        "                    \r\n",
        "        print ('writing last rule_collection to file: ' + str(k))\r\n",
        "        total_rules += rule_collection.size()\r\n",
        "        rule_collection.remove_redundancy(self.freq_itemset_dict)\r\n",
        "        rule_collection.save(output_file_name, True)\r\n",
        "        remaining_rules += rule_collection.size()\r\n",
        "        rule_collection.clear()\r\n",
        "        \r\n",
        "        print ('Finish for sub frequent item-sets!!!')\r\n",
        "        print ('Number of redundant rules ' + str(total_rules - remaining_rules) + '/' + str(total_rules))\r\n",
        "                  \r\n",
        "    '''\r\n",
        "    Generate association rules for whole data-set\r\n",
        "    '''  \r\n",
        "    def execute(self, output_file_name):\r\n",
        "        \r\n",
        "        itemset_chunks = self.freq_itemset_dict.split(self.nthreads)\r\n",
        "        \r\n",
        "        processes = []\r\n",
        "        for index in range(self.nthreads):\r\n",
        "            file_name = output_file_name + '.' + str(index)\r\n",
        "            process_i = Process(target=self.generate_rules, \r\n",
        "                                args=(itemset_chunks[index], file_name))\r\n",
        "            processes.append(process_i)\r\n",
        "            \r\n",
        "            \r\n",
        "        for process_i in processes:\r\n",
        "            process_i.start()\r\n",
        "            \r\n",
        "        # wait for all thread completes\r\n",
        "        for process_i in processes:\r\n",
        "            process_i.join()\r\n",
        "            \r\n",
        "        print ('Finish generating rules!!!!')    \r\n",
        "            \r\n",
        "            "
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QIXGxJU2AT"
      },
      "source": [
        "Argumetns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsdEy9woU3D6"
      },
      "source": [
        "class ARMParams(object):\r\n",
        "    '''\r\n",
        "    classdocs\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, minsup, minconf, itemset_max_size=-1):\r\n",
        "        '''\r\n",
        "        Constructor\r\n",
        "        '''\r\n",
        "        self.min_sup = minsup \r\n",
        "        self.min_conf = minconf\r\n",
        "        self.itemset_max_size = itemset_max_size"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ9NUuDaW2ZS"
      },
      "source": [
        "class ARMFiles(object):\r\n",
        "    '''\r\n",
        "    classdocs\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, default_folder = '/content/'):\r\n",
        "        '''\r\n",
        "        Constructor\r\n",
        "        '''\r\n",
        "        self.temp_folder = default_folder\r\n",
        "        \r\n",
        "        self.itemset_tmp_file = self.temp_folder + 'miner.tmp.itemsets'\r\n",
        "        self.rules_tmp_file = self.temp_folder + 'miner.tmp.rules'\r\n",
        "      \r\n",
        "  \r\n",
        "        \r\n",
        "        self.feature_tmp_file = self.temp_folder +'miner.tmp.features'\r\n",
        "        self.non_redundant_rule_tmp_file = self.temp_folder +'miner.tmp.non_redundant_rules'\r\n",
        "        self.non_redundant_rule_feature_tmp_file = self.temp_folder + 'miner.tmp.non_redundant_rules.features'\r\n",
        "        "
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIgcGyPtSo9c"
      },
      "source": [
        "getting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm2czXkpVq2i"
      },
      "source": [
        "class DataSet:\r\n",
        "    def __init__(self):\r\n",
        "        self.current = 0\r\n",
        "        self.train_data = []\r\n",
        "        self.data_labels = []\r\n",
        "        \r\n",
        "    \r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.train_data)\r\n",
        "                \r\n",
        "    def size(self):\r\n",
        "        return len(self.train_data)\r\n",
        "    \r\n",
        "    def get_transaction(self, index):\r\n",
        "        return self.train_data[index]\r\n",
        "    \r\n",
        "    def clear(self):\r\n",
        "        self.train_data.clear()\r\n",
        "        \r\n",
        "    def add_transaction(self, t):\r\n",
        "        return self.train_data.append(t)\r\n",
        "\r\n",
        "    '''\r\n",
        "    Load data set from a file. The input file must be formated in CSV (comma separated)\r\n",
        "    class_index is used in the case of data-set with labels. \r\n",
        "    '''\r\n",
        "    def load(self, file_path, class_index = -1, has_header = False):\r\n",
        "        self.train_data = []\r\n",
        "        if class_index != -1: self.data_labels = []\r\n",
        "        \r\n",
        "        with open(file_path, \"r\") as text_in_file:\r\n",
        "            if has_header == True:\r\n",
        "                text_in_file.readline()\r\n",
        "                \r\n",
        "            for line in text_in_file:\r\n",
        "                #print(\"dataset script line\", line)\r\n",
        "                transaction = [x.strip() for x in line.split(',')]\r\n",
        "                transaction = list(filter(None, transaction))\r\n",
        "                #print(\"datset script transaction\" , transaction)\r\n",
        "                \r\n",
        "                if (class_index != -1):\r\n",
        "                    self.data_labels.append(transaction[class_index])\r\n",
        "                    del transaction[class_index]\r\n",
        "                \r\n",
        "                self.train_data.append(list(set(transaction)))\r\n",
        "        print(\"Loading done\")"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca8Ntp5_cuRY"
      },
      "source": [
        "Getting data as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYuAvMhfTi_r"
      },
      "source": [
        "train_data_set = DataSet()"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuBpWAwnVboC",
        "outputId": "2c442f9b-8e09-49bf-b0d1-2cb27a5f2a05"
      },
      "source": [
        "train_data_set.load(\"/content/breast_train_transactions.txt\", -1)"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGX4ldtR5Cz"
      },
      "source": [
        "rule_miner = RuleMiner('spect', train_data_set)"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tIL5ykcxLw"
      },
      "source": [
        "Give arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irg2malsU_sj"
      },
      "source": [
        "minsup, minconf, itemset_max_size = .1,.3,-1"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4vqPthqUK-D"
      },
      "source": [
        "arm_params = ARMParams(minsup, minconf, itemset_max_size)"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ryoDPZczZC"
      },
      "source": [
        "Generates frequent item sets and rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TYH5f67Sioc",
        "outputId": "e786bd33-438d-486d-c697-e35767ca06f8"
      },
      "source": [
        "rule_miner.generate_itemsets_and_rules(arm_params)"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating frequent item-sets...\n",
            "size of data-set: 240\n",
            "get frequent item sets with 1 item\n",
            "extracting item-sets with 2 items ....\n",
            "generate candidates with 2 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 142\n",
            "extracting item-sets with 3 items ....\n",
            "generate candidates with 3 items\n",
            "generate candidates with 3 items\n",
            "generate candidates with 3 items\n",
            "generate candidates with 3 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 240\n",
            "extracting item-sets with 4 items ....\n",
            "generate candidates with 4 items\n",
            "generate candidates with 4 items\n",
            "generate candidates with 4 items\n",
            "generate candidates with 4 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 220\n",
            "extracting item-sets with 5 items ....\n",
            "generate candidates with 5 items\n",
            "generate candidates with 5 items\n",
            "generate candidates with 5 items\n",
            "generate candidates with 5 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 96\n",
            "extracting item-sets with 6 items ....\n",
            "generate candidates with 6 items\n",
            "generate candidates with 6 items\n",
            "generate candidates with 6 items\n",
            "generate candidates with 6 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 16\n",
            "extracting item-sets with 7 items ....\n",
            "generate candidates with 7 items\n",
            "generate candidates with 7 items\n",
            "generate candidates with 7 items\n",
            "generate candidates with 7 items\n",
            "Writing frequent itemset to file....\n",
            "#item-sets: 0\n",
            "stop at k = 8\n",
            "generating rules ....\n",
            "Number of frequent item-sets: 741\n",
            "clear old file...\n",
            "clear old file...\n",
            "writing last rule_collection to file: 32\n",
            "clear old file...\n",
            "Finish for sub frequent item-sets!!!\n",
            "Number of redundant rules 0/27\n",
            "writing last rule_collection to file: 88\n",
            "writing last rule_collection to file: 56\n",
            "Finish for sub frequent item-sets!!!\n",
            "Finish for sub frequent item-sets!!!\n",
            "Number of redundant rules 3/84\n",
            "clear old file...\n",
            "Number of redundant rules 0/52\n",
            "writing last rule_collection to file: 117\n",
            "Finish for sub frequent item-sets!!!\n",
            "Number of redundant rules 16/116\n",
            "Finish generating rules!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}