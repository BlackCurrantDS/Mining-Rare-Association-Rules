{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RPP_Processing_each_file_Hep.ipynb",
      "provenance": [],
      "mount_file_id": "1Jcw9aGDbF3qtzLKKY12MTMc1-tAh2Qcu",
      "authorship_tag": "ABX9TyOXLbaf7tDLLO38pa7RF8Fl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackCurrantDS/DBSE_Project/blob/main/RPP_Processing_each_file_Hep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIHaRw5eHFdO"
      },
      "source": [
        "Generating rule for each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EexubHst1mvu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import fileinput"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrssKvI-1XYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c404463-7845-474a-d3a2-f578ba47f08a"
      },
      "source": [
        "#for dictionary\n",
        "originalfile = \"/content/drive/MyDrive/DataSets_for_Paper/hep_data_original.csv\"\n",
        "\n",
        "df = pd.read_csv(originalfile)\n",
        "\n",
        "#removing index column\n",
        "cols = [0]\n",
        "df.drop(df.columns[cols],axis=1,inplace=True)\n",
        "\n",
        "for i,n in enumerate(df.columns):\n",
        "      if n=='class':\n",
        "        df[n] = 'class@' + df[n].astype(str)\n",
        "      else:\n",
        "        df[n] = 'A'+str(i)+'@' + df[n].astype(str)\n",
        "\n",
        "uni = pd.DataFrame()\n",
        "\n",
        "for col in df:\n",
        "  uni= pd.concat([uni, pd.DataFrame(df[col].unique())], ignore_index=True)\n",
        "\n",
        "my_dict = uni.to_dict()\n",
        "\n",
        "my_dict = my_dict[0]\n",
        "\n",
        "my_dict\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'class@2',\n",
              " 1: 'class@1',\n",
              " 2: 'A1@30',\n",
              " 3: 'A1@50',\n",
              " 4: 'A1@78',\n",
              " 5: 'A1@31',\n",
              " 6: 'A1@34',\n",
              " 7: 'A1@51',\n",
              " 8: 'A1@23',\n",
              " 9: 'A1@39',\n",
              " 10: 'A1@32',\n",
              " 11: 'A1@41',\n",
              " 12: 'A1@47',\n",
              " 13: 'A1@38',\n",
              " 14: 'A1@66',\n",
              " 15: 'A1@40',\n",
              " 16: 'A1@22',\n",
              " 17: 'A1@27',\n",
              " 18: 'A1@42',\n",
              " 19: 'A1@25',\n",
              " 20: 'A1@49',\n",
              " 21: 'A1@58',\n",
              " 22: 'A1@61',\n",
              " 23: 'A1@62',\n",
              " 24: 'A1@26',\n",
              " 25: 'A1@35',\n",
              " 26: 'A1@37',\n",
              " 27: 'A1@20',\n",
              " 28: 'A1@65',\n",
              " 29: 'A1@52',\n",
              " 30: 'A1@33',\n",
              " 31: 'A1@56',\n",
              " 32: 'A1@28',\n",
              " 33: 'A1@36',\n",
              " 34: 'A1@44',\n",
              " 35: 'A1@64',\n",
              " 36: 'A1@45',\n",
              " 37: 'A1@57',\n",
              " 38: 'A1@24',\n",
              " 39: 'A1@67',\n",
              " 40: 'A1@59',\n",
              " 41: 'A1@60',\n",
              " 42: 'A1@48',\n",
              " 43: 'A1@54',\n",
              " 44: 'A1@7',\n",
              " 45: 'A1@69',\n",
              " 46: 'A1@72',\n",
              " 47: 'A1@70',\n",
              " 48: 'A1@46',\n",
              " 49: 'A1@53',\n",
              " 50: 'A1@43',\n",
              " 51: 'A2@2',\n",
              " 52: 'A2@1',\n",
              " 53: 'A3@1',\n",
              " 54: 'A3@2',\n",
              " 55: 'A3@?',\n",
              " 56: 'A4@2',\n",
              " 57: 'A4@1',\n",
              " 58: 'A5@2',\n",
              " 59: 'A5@1',\n",
              " 60: 'A5@?',\n",
              " 61: 'A6@2',\n",
              " 62: 'A6@1',\n",
              " 63: 'A6@?',\n",
              " 64: 'A7@2',\n",
              " 65: 'A7@1',\n",
              " 66: 'A7@?',\n",
              " 67: 'A8@1',\n",
              " 68: 'A8@2',\n",
              " 69: 'A8@?',\n",
              " 70: 'A9@2',\n",
              " 71: 'A9@1',\n",
              " 72: 'A9@?',\n",
              " 73: 'A10@2',\n",
              " 74: 'A10@1',\n",
              " 75: 'A10@?',\n",
              " 76: 'A11@2',\n",
              " 77: 'A11@1',\n",
              " 78: 'A11@?',\n",
              " 79: 'A12@2',\n",
              " 80: 'A12@1',\n",
              " 81: 'A12@?',\n",
              " 82: 'A13@2',\n",
              " 83: 'A13@?',\n",
              " 84: 'A13@1',\n",
              " 85: 'A14@1.00',\n",
              " 86: 'A14@0.90',\n",
              " 87: 'A14@0.70',\n",
              " 88: 'A14@?',\n",
              " 89: 'A14@1.30',\n",
              " 90: 'A14@2.20',\n",
              " 91: 'A14@2.00',\n",
              " 92: 'A14@1.20',\n",
              " 93: 'A14@0.60',\n",
              " 94: 'A14@0.40',\n",
              " 95: 'A14@0.80',\n",
              " 96: 'A14@1.40',\n",
              " 97: 'A14@2.30',\n",
              " 98: 'A14@0.50',\n",
              " 99: 'A14@0.30',\n",
              " 100: 'A14@4.60',\n",
              " 101: 'A14@1.80',\n",
              " 102: 'A14@3.50',\n",
              " 103: 'A14@4.10',\n",
              " 104: 'A14@1.60',\n",
              " 105: 'A14@2.80',\n",
              " 106: 'A14@1.50',\n",
              " 107: 'A14@2.50',\n",
              " 108: 'A14@3.00',\n",
              " 109: 'A14@4.80',\n",
              " 110: 'A14@2.40',\n",
              " 111: 'A14@1.70',\n",
              " 112: 'A14@1.10',\n",
              " 113: 'A14@3.20',\n",
              " 114: 'A14@2.90',\n",
              " 115: 'A14@8.00',\n",
              " 116: 'A14@3.90',\n",
              " 117: 'A14@1.90',\n",
              " 118: 'A14@4.20',\n",
              " 119: 'A14@7.60',\n",
              " 120: 'A15@85',\n",
              " 121: 'A15@135',\n",
              " 122: 'A15@96',\n",
              " 123: 'A15@46',\n",
              " 124: 'A15@?',\n",
              " 125: 'A15@95',\n",
              " 126: 'A15@78',\n",
              " 127: 'A15@59',\n",
              " 128: 'A15@81',\n",
              " 129: 'A15@57',\n",
              " 130: 'A15@72',\n",
              " 131: 'A15@102',\n",
              " 132: 'A15@62',\n",
              " 133: 'A15@53',\n",
              " 134: 'A15@70',\n",
              " 135: 'A15@48',\n",
              " 136: 'A15@133',\n",
              " 137: 'A15@60',\n",
              " 138: 'A15@45',\n",
              " 139: 'A15@175',\n",
              " 140: 'A15@280',\n",
              " 141: 'A15@58',\n",
              " 142: 'A15@67',\n",
              " 143: 'A15@194',\n",
              " 144: 'A15@150',\n",
              " 145: 'A15@180',\n",
              " 146: 'A15@75',\n",
              " 147: 'A15@56',\n",
              " 148: 'A15@71',\n",
              " 149: 'A15@74',\n",
              " 150: 'A15@80',\n",
              " 151: 'A15@191',\n",
              " 152: 'A15@125',\n",
              " 153: 'A15@110',\n",
              " 154: 'A15@50',\n",
              " 155: 'A15@92',\n",
              " 156: 'A15@52',\n",
              " 157: 'A15@26',\n",
              " 158: 'A15@215',\n",
              " 159: 'A15@164',\n",
              " 160: 'A15@103',\n",
              " 161: 'A15@34',\n",
              " 162: 'A15@68',\n",
              " 163: 'A15@82',\n",
              " 164: 'A15@127',\n",
              " 165: 'A15@76',\n",
              " 166: 'A15@100',\n",
              " 167: 'A15@55',\n",
              " 168: 'A15@167',\n",
              " 169: 'A15@30',\n",
              " 170: 'A15@179',\n",
              " 171: 'A15@141',\n",
              " 172: 'A15@44',\n",
              " 173: 'A15@165',\n",
              " 174: 'A15@118',\n",
              " 175: 'A15@230',\n",
              " 176: 'A15@107',\n",
              " 177: 'A15@40',\n",
              " 178: 'A15@147',\n",
              " 179: 'A15@114',\n",
              " 180: 'A15@84',\n",
              " 181: 'A15@123',\n",
              " 182: 'A15@168',\n",
              " 183: 'A15@86',\n",
              " 184: 'A15@138',\n",
              " 185: 'A15@155',\n",
              " 186: 'A15@63',\n",
              " 187: 'A15@256',\n",
              " 188: 'A15@119',\n",
              " 189: 'A15@139',\n",
              " 190: 'A15@90',\n",
              " 191: 'A15@160',\n",
              " 192: 'A15@158',\n",
              " 193: 'A15@115',\n",
              " 194: 'A15@243',\n",
              " 195: 'A15@181',\n",
              " 196: 'A15@130',\n",
              " 197: 'A15@166',\n",
              " 198: 'A15@295',\n",
              " 199: 'A15@120',\n",
              " 200: 'A15@65',\n",
              " 201: 'A15@109',\n",
              " 202: 'A15@89',\n",
              " 203: 'A15@126',\n",
              " 204: 'A16@18',\n",
              " 205: 'A16@42',\n",
              " 206: 'A16@32',\n",
              " 207: 'A16@52',\n",
              " 208: 'A16@200',\n",
              " 209: 'A16@28',\n",
              " 210: 'A16@?',\n",
              " 211: 'A16@48',\n",
              " 212: 'A16@120',\n",
              " 213: 'A16@30',\n",
              " 214: 'A16@249',\n",
              " 215: 'A16@60',\n",
              " 216: 'A16@144',\n",
              " 217: 'A16@89',\n",
              " 218: 'A16@53',\n",
              " 219: 'A16@166',\n",
              " 220: 'A16@20',\n",
              " 221: 'A16@98',\n",
              " 222: 'A16@63',\n",
              " 223: 'A16@46',\n",
              " 224: 'A16@55',\n",
              " 225: 'A16@25',\n",
              " 226: 'A16@58',\n",
              " 227: 'A16@29',\n",
              " 228: 'A16@92',\n",
              " 229: 'A16@150',\n",
              " 230: 'A16@68',\n",
              " 231: 'A16@14',\n",
              " 232: 'A16@16',\n",
              " 233: 'A16@90',\n",
              " 234: 'A16@86',\n",
              " 235: 'A16@110',\n",
              " 236: 'A16@80',\n",
              " 237: 'A16@420',\n",
              " 238: 'A16@44',\n",
              " 239: 'A16@65',\n",
              " 240: 'A16@145',\n",
              " 241: 'A16@31',\n",
              " 242: 'A16@78',\n",
              " 243: 'A16@59',\n",
              " 244: 'A16@38',\n",
              " 245: 'A16@75',\n",
              " 246: 'A16@64',\n",
              " 247: 'A16@54',\n",
              " 248: 'A16@43',\n",
              " 249: 'A16@33',\n",
              " 250: 'A16@15',\n",
              " 251: 'A16@39',\n",
              " 252: 'A16@182',\n",
              " 253: 'A16@271',\n",
              " 254: 'A16@45',\n",
              " 255: 'A16@100',\n",
              " 256: 'A16@242',\n",
              " 257: 'A16@24',\n",
              " 258: 'A16@224',\n",
              " 259: 'A16@69',\n",
              " 260: 'A16@156',\n",
              " 261: 'A16@123',\n",
              " 262: 'A16@117',\n",
              " 263: 'A16@157',\n",
              " 264: 'A16@128',\n",
              " 265: 'A16@23',\n",
              " 266: 'A16@40',\n",
              " 267: 'A16@227',\n",
              " 268: 'A16@269',\n",
              " 269: 'A16@34',\n",
              " 270: 'A16@648',\n",
              " 271: 'A16@225',\n",
              " 272: 'A16@136',\n",
              " 273: 'A16@81',\n",
              " 274: 'A16@153',\n",
              " 275: 'A16@118',\n",
              " 276: 'A16@231',\n",
              " 277: 'A16@101',\n",
              " 278: 'A16@278',\n",
              " 279: 'A16@49',\n",
              " 280: 'A16@181',\n",
              " 281: 'A16@140',\n",
              " 282: 'A16@70',\n",
              " 283: 'A16@114',\n",
              " 284: 'A16@173',\n",
              " 285: 'A16@528',\n",
              " 286: 'A16@152',\n",
              " 287: 'A16@142',\n",
              " 288: 'A16@19',\n",
              " 289: 'A17@4.0',\n",
              " 290: 'A17@3.5',\n",
              " 291: 'A17@?',\n",
              " 292: 'A17@4.4',\n",
              " 293: 'A17@3.9',\n",
              " 294: 'A17@3.7',\n",
              " 295: 'A17@4.9',\n",
              " 296: 'A17@2.9',\n",
              " 297: 'A17@4.3',\n",
              " 298: 'A17@4.1',\n",
              " 299: 'A17@4.2',\n",
              " 300: 'A17@4.7',\n",
              " 301: 'A17@3.8',\n",
              " 302: 'A17@2.7',\n",
              " 303: 'A17@4.6',\n",
              " 304: 'A17@5.0',\n",
              " 305: 'A17@3.3',\n",
              " 306: 'A17@4.5',\n",
              " 307: 'A17@3.4',\n",
              " 308: 'A17@3.1',\n",
              " 309: 'A17@3.0',\n",
              " 310: 'A17@2.6',\n",
              " 311: 'A17@5.3',\n",
              " 312: 'A17@4.8',\n",
              " 313: 'A17@2.8',\n",
              " 314: 'A17@3.6',\n",
              " 315: 'A17@2.1',\n",
              " 316: 'A17@6.4',\n",
              " 317: 'A17@2.4',\n",
              " 318: 'A17@2.2',\n",
              " 319: 'A18@?',\n",
              " 320: 'A18@80',\n",
              " 321: 'A18@75',\n",
              " 322: 'A18@85',\n",
              " 323: 'A18@54',\n",
              " 324: 'A18@52',\n",
              " 325: 'A18@78',\n",
              " 326: 'A18@46',\n",
              " 327: 'A18@63',\n",
              " 328: 'A18@62',\n",
              " 329: 'A18@64',\n",
              " 330: 'A18@39',\n",
              " 331: 'A18@100',\n",
              " 332: 'A18@47',\n",
              " 333: 'A18@70',\n",
              " 334: 'A18@36',\n",
              " 335: 'A18@40',\n",
              " 336: 'A18@74',\n",
              " 337: 'A18@60',\n",
              " 338: 'A18@73',\n",
              " 339: 'A18@90',\n",
              " 340: 'A18@21',\n",
              " 341: 'A18@77',\n",
              " 342: 'A18@29',\n",
              " 343: 'A18@41',\n",
              " 344: 'A18@66',\n",
              " 345: 'A18@57',\n",
              " 346: 'A18@56',\n",
              " 347: 'A18@76',\n",
              " 348: 'A18@58',\n",
              " 349: 'A18@84',\n",
              " 350: 'A18@38',\n",
              " 351: 'A18@67',\n",
              " 352: 'A18@31',\n",
              " 353: 'A18@51',\n",
              " 354: 'A18@23',\n",
              " 355: 'A18@72',\n",
              " 356: 'A18@32',\n",
              " 357: 'A18@30',\n",
              " 358: 'A18@0',\n",
              " 359: 'A18@50',\n",
              " 360: 'A18@43',\n",
              " 361: 'A18@35',\n",
              " 362: 'A18@48',\n",
              " 363: 'A18@42',\n",
              " 364: 'A19@1',\n",
              " 365: 'A19@2'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv_dl9RS3s1F"
      },
      "source": [
        "mapping = {\n",
        "    \n",
        "    'A10':'SPLEEN PALPABLE',\n",
        "    'A11':'SPIDERS',\n",
        "    'A12':'ASCITES',\n",
        "    'A13':'VARICES',\n",
        "    'A14':'ALK PHOSPHATE',\n",
        "    'A15':'SGOT',\n",
        "    'A16':'ALBUMIN',\n",
        "    'A17':'PROTIME',\n",
        "    'A18':'HISTOLOGY',\n",
        "    \n",
        "\t'A0':'class',\n",
        "    'A1':'AGE',\n",
        "    'A2':'SEX',\n",
        "    'A3':'STEROID',\n",
        "    'A4':'ANTIVIRALS',\n",
        "    'A5':'FATIGUE',\n",
        "    'A6':'MALAISE',\n",
        "    'A7':'ANOREXIA',\n",
        "    'A8':'LIVER BIG',\n",
        "    'A9':'LIVER FIRM'\n",
        "           \n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi2_-x6T4SoV"
      },
      "source": [
        "#min_support = np.arange(0.01, .11, 0.01)\n",
        "min_support = [.01]\n",
        "d= \"/content/drive/MyDrive/Hep_DataSet/RPP/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXluIZqe44dN"
      },
      "source": [
        "def parse_input(filename):\n",
        "                with open(filename) as f:\n",
        "                    data = [set(literal_eval(line)) for line in f]\n",
        "                return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7u_vqchHI6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a42387-b74a-4d9b-e126-91f00d2f0384"
      },
      "source": [
        "import os\n",
        "from ast import literal_eval\n",
        "\n",
        "for i in (min_support):\n",
        "          print(round(i,2))\n",
        "          print(str(round(i,2)).replace('0.', ''))\n",
        "          file_to_process = d+\"output\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          superset_file = d+\"superset\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          superset_file_final = d+\"superset\"+str(round(i,2)).replace('0.', '')+\"_final\"+\".txt\"\n",
        "          superset_file_final_v1 = d+\"superset\"+str(round(i,2)).replace('0.', '')+\"_final_v1\"+\".txt\"\n",
        "          rule_file = d+\"output_rules\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          rule_file_final = d+\"output_rules_final_\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          \n",
        "          \n",
        "\n",
        "          rpp_process = pd.read_csv(file_to_process, header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          temp_df = pd.concat([rpp_process[['itemsets']], rpp_process['itemsets'].str.split(' ', expand=True)], axis=1)\n",
        "\n",
        "          temp_df=temp_df.drop('itemsets', axis=1)\n",
        "\n",
        "        #for class , moving the class column as first\n",
        "          with open(\"/content/temp_file.csv\", 'w') as f:\n",
        "                for row in temp_df.itertuples(index=False):\n",
        "                    #print(row)\n",
        "                    ls = list(row)\n",
        "                    ls = [x for x in ls if x]\n",
        "                    try:\n",
        "                      if '0' or '1' in ls:\n",
        "                            old_index = ls.index('0') if '0' in ls else ls.index('1')\n",
        "                            #print(\"old_index\", old_index)\n",
        "                            \n",
        "                            ls.insert(0, ls.pop(old_index))\n",
        "                            #print(\"after row\", ls)\n",
        "                            f.write(str(ls)+'\\n')\n",
        "                    except ValueError:\n",
        "                      pass\n",
        "\n",
        "          def compute_output(output_file, data, filter_value):\n",
        "              ls = data\n",
        "              index_to_pop=[]\n",
        "              for set1 in ls:\n",
        "                #print(\"set1\", set1)\n",
        "                for set2 in ls:\n",
        "                    if set1 is set2:\n",
        "                        # Do not try to compare a row with itself\n",
        "                        continue\n",
        "                    elif len(set(set1).difference(set(set2))) == 0:\n",
        "                          if set1 in ls:\n",
        "                            index = ls.index(set1)\n",
        "                            index_to_pop.append(index)\n",
        "\n",
        "                            break\n",
        "              print(\"Final index_to_pop\",index_to_pop)              \n",
        "              for index in sorted(index_to_pop, reverse=True):\n",
        "                  del ls[index]           \n",
        "              print(\"Final list\",ls)\n",
        "\n",
        "              f = open(superset_file,'w')\n",
        "              for line in ls:\n",
        "                f.write(str(line)+'\\n')\n",
        "              f.close()\n",
        "\n",
        "          def filter_file(path, filter_value=3, in_name='temp_file.csv', out_name='filteredSets'):\n",
        "              data = parse_input(os.path.join(path, in_name))\n",
        "              print(\"data\", data)\n",
        "              output_filename = os.path.join(path, '{}{}'.format(out_name, filter_value))\n",
        "              with open(output_filename, 'w') as out_file:\n",
        "                  compute_output(out_file, data, filter_value)\n",
        "\n",
        "          filter_file('/content')\n",
        "\n",
        "          print(\"longest ones sorted\")\n",
        "\n",
        "          with open(superset_file) as filein, open(superset_file_final,'w') as fileout:\n",
        "                    for line in filein:\n",
        "                            line=line.replace(\"'\",\"\")\n",
        "                            line=line.replace(\"}\",\"\")\n",
        "                            line=line.replace(\"{\",\"\")\n",
        "                            line=line.replace(\", \",\" \")\n",
        "                            fileout.write(line)\n",
        "\n",
        "            \n",
        "          with open(superset_file_final) as filein, open(superset_file_final_v1,'w') as fileout:\n",
        "              for line in filein:\n",
        "                  line=line.replace(\"[\",\"\")\n",
        "                  line=line.replace(\"]\",\"\")\n",
        "                  fileout.write(line)\n",
        "\n",
        "          rpp_process = pd.read_csv(superset_file_final_v1, header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          temp_df = pd.concat([rpp_process[['itemsets']], rpp_process['itemsets'].str.split(' ', expand=True)], axis=1)\n",
        "\n",
        "          temp_df=temp_df.drop('itemsets', axis=1)\n",
        "\n",
        "          with open(superset_file, 'w') as f:\n",
        "                  for row in temp_df.itertuples(index=False):\n",
        "                      ls = list(row)\n",
        "                      ls = [x for x in ls if x]\n",
        "                      try:\n",
        "                        if '0' or '1' in ls:\n",
        "                              old_index = ls.index('0') if '0' in ls else ls.index('1')\n",
        "                              #print(\"old_index\", old_index)\n",
        "                              \n",
        "                              ls.insert(0, ls.pop(old_index))\n",
        "                              #print(\"after row\", ls)\n",
        "                              f.write(str(ls)+'\\n')\n",
        "                      except ValueError:\n",
        "                        pass\n",
        "\n",
        "          with open(superset_file) as filein, open(superset_file_final,'w') as fileout:\n",
        "                    for line in filein:\n",
        "                            line=line.replace(\"'\",\"\")\n",
        "                            line=line.replace(\"}\",\"\")\n",
        "                            line=line.replace(\"{\",\"\")\n",
        "                            line=line.replace(\", \",\" \")\n",
        "                            fileout.write(line)\n",
        "\n",
        "            \n",
        "          with open(superset_file_final) as filein, open(superset_file_final_v1,'w') as fileout:\n",
        "              for line in filein:\n",
        "                  line=line.replace(\"[\",\"\")\n",
        "                  line=line.replace(\"]\",\"\")\n",
        "                  fileout.write(line)\n",
        "\n",
        "          superset = pd.read_csv(superset_file_final_v1,header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          superset_df = pd.concat([superset[['itemsets']], superset['itemsets'].str.split(' ', expand=True)], axis=1\n",
        "                  )\n",
        "            \n",
        "          superset_df.to_csv(\"superset_df.csv\", header=None, index=False)\n",
        "\n",
        "          print(\"Superset is done\")\n",
        "\n",
        "            \n",
        "            #superset_df = superset_df.loc[superset_df.iloc[:,1].isin(['1','0'])] #filtering rows\n",
        "\n",
        "          superset_df = superset_df.loc[superset_df.iloc[:,1].isin(['1'])] #filtering only for minority\n",
        "\n",
        "          superset_df.to_csv(\"superset_df_1.csv\", header=None, index=False)\n",
        "\n",
        "          superset_df=superset_df.drop('itemsets', axis=1)\n",
        "\n",
        "          print(\"lenght of columns\", len(superset_df.columns))\n",
        "          if len(superset_df.columns) ==9:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g', 'h', 'i']\n",
        "          elif len(superset_df.columns) ==8:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g', 'h']\n",
        "\n",
        "          elif len(superset_df.columns) ==7:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g']\n",
        "\n",
        "          elif len(superset_df.columns) ==6:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f']\n",
        "\n",
        "          elif len(superset_df.columns) ==5:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "          for i,n in enumerate(superset_df.columns):\n",
        "  \n",
        "                    superset_df[n] = '@' + superset_df[n].astype(str)\n",
        "\n",
        "          di = {f'@{k}': v for k, v in my_dict.items()}\n",
        "\n",
        "            \n",
        "          for col in superset_df:\n",
        "              superset_df[col] = superset_df[col].replace(di)\n",
        "\n",
        "          print(\"Mapping is done\")\n",
        "          superset_df.to_csv(\"superset_df_1_after_mapping.csv\", header=None, index=False) \n",
        "          if len(superset_df.columns) ==9:\n",
        "\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']+','+superset_df['h']+','+superset_df['i']\n",
        "          elif len(superset_df.columns) ==8:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']+','+superset_df['h']\n",
        "          \n",
        "          elif len(superset_df.columns) ==7:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']\n",
        "          \n",
        "          elif len(superset_df.columns) ==6:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']\n",
        "          \n",
        "          elif len(superset_df.columns) ==5:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          superset_df['pos'] = superset_df['rules'].str.find('@None')\n",
        "          superset_df['rules'] = superset_df.apply(lambda x: x['rules'][0:x['pos']],axis=1)\n",
        "          superset_df['rules'] = superset_df['rules']+\">\"+superset_df['a']\n",
        "          superset_df['rules'] = superset_df['rules'].str.replace(',@,','')\n",
        "          superset_df['rules'] = superset_df['rules'].str.replace(',>','>')\n",
        "\n",
        "          superset_df['rules'].to_csv(rule_file_final, header=None, index=False)\n",
        "          #for classifer file\n",
        "          superset_df['rules'].to_csv(rule_file, header=None, index=False)\n",
        "\n",
        "          print(\"Rules generated\")\n",
        "          text = rule_file_final\n",
        "          fields = mapping\n",
        "\n",
        "          lst = []\n",
        "          for line in fileinput.input(text):\n",
        "              for field in fields:\n",
        "                  if field in line:\n",
        "                      line = line.replace(field, fields[field])\n",
        "              lst.append(line)\n",
        "\n",
        "          print(lst)\n",
        "\n",
        "          f = open(rule_file_final,'w')\n",
        "          for line in lst:\n",
        "              f.write(line)\n",
        "          f.close()\n",
        "\n",
        "          print(\"Rules mapping done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01\n",
            "01\n",
            "data "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}