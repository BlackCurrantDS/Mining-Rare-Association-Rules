{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FP_Processing_each_file_hep_without_superset_v1.ipynb",
      "provenance": [],
      "mount_file_id": "1Jcw9aGDbF3qtzLKKY12MTMc1-tAh2Qcu",
      "authorship_tag": "ABX9TyPyMBymILj9dIaNMgazcjbD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackCurrantDS/Mining-Rare-Association-Rules/blob/master/FP_Processing_each_file_hep_without_superset_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIHaRw5eHFdO"
      },
      "source": [
        "Generating rule for each file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EexubHst1mvu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import fileinput"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrssKvI-1XYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b13f631-9132-4d77-82c9-42f1e4fe913a"
      },
      "source": [
        "#for dictionary\n",
        "originalfile = \"/content/hep_data_original.csv\"\n",
        "\n",
        "df = pd.read_csv(originalfile)\n",
        "\n",
        "#removing index column\n",
        "cols = [0]\n",
        "df.drop(df.columns[cols],axis=1,inplace=True)\n",
        "\n",
        "df = df.drop(columns=['BILIRUBIN'])\n",
        "\n",
        "for i,n in enumerate(df.columns):\n",
        "      if n=='class':\n",
        "        df[n] = 'class@' + df[n].astype(str)\n",
        "      else:\n",
        "        df[n] = 'A'+str(i)+'@' + df[n].astype(str)\n",
        "\n",
        "uni = pd.DataFrame()\n",
        "\n",
        "for col in df:\n",
        "  uni= pd.concat([uni, pd.DataFrame(df[col].unique())], ignore_index=True)\n",
        "\n",
        "my_dict = uni.to_dict()\n",
        "\n",
        "my_dict = my_dict[0]\n",
        "\n",
        "my_dict\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'class@2',\n",
              " 1: 'class@1',\n",
              " 2: 'A1@30',\n",
              " 3: 'A1@50',\n",
              " 4: 'A1@78',\n",
              " 5: 'A1@31',\n",
              " 6: 'A1@34',\n",
              " 7: 'A1@51',\n",
              " 8: 'A1@23',\n",
              " 9: 'A1@39',\n",
              " 10: 'A1@32',\n",
              " 11: 'A1@41',\n",
              " 12: 'A1@47',\n",
              " 13: 'A1@38',\n",
              " 14: 'A1@66',\n",
              " 15: 'A1@40',\n",
              " 16: 'A1@22',\n",
              " 17: 'A1@27',\n",
              " 18: 'A1@42',\n",
              " 19: 'A1@25',\n",
              " 20: 'A1@49',\n",
              " 21: 'A1@58',\n",
              " 22: 'A1@61',\n",
              " 23: 'A1@62',\n",
              " 24: 'A1@26',\n",
              " 25: 'A1@35',\n",
              " 26: 'A1@37',\n",
              " 27: 'A1@20',\n",
              " 28: 'A1@65',\n",
              " 29: 'A1@52',\n",
              " 30: 'A1@33',\n",
              " 31: 'A1@56',\n",
              " 32: 'A1@28',\n",
              " 33: 'A1@36',\n",
              " 34: 'A1@44',\n",
              " 35: 'A1@64',\n",
              " 36: 'A1@45',\n",
              " 37: 'A1@57',\n",
              " 38: 'A1@24',\n",
              " 39: 'A1@67',\n",
              " 40: 'A1@59',\n",
              " 41: 'A1@60',\n",
              " 42: 'A1@48',\n",
              " 43: 'A1@54',\n",
              " 44: 'A1@7',\n",
              " 45: 'A1@69',\n",
              " 46: 'A1@72',\n",
              " 47: 'A1@70',\n",
              " 48: 'A1@46',\n",
              " 49: 'A1@53',\n",
              " 50: 'A1@43',\n",
              " 51: 'A2@2',\n",
              " 52: 'A2@1',\n",
              " 53: 'A3@1',\n",
              " 54: 'A3@2',\n",
              " 55: 'A3@?',\n",
              " 56: 'A4@2',\n",
              " 57: 'A4@1',\n",
              " 58: 'A5@2',\n",
              " 59: 'A5@1',\n",
              " 60: 'A5@?',\n",
              " 61: 'A6@2',\n",
              " 62: 'A6@1',\n",
              " 63: 'A6@?',\n",
              " 64: 'A7@2',\n",
              " 65: 'A7@1',\n",
              " 66: 'A7@?',\n",
              " 67: 'A8@1',\n",
              " 68: 'A8@2',\n",
              " 69: 'A8@?',\n",
              " 70: 'A9@2',\n",
              " 71: 'A9@1',\n",
              " 72: 'A9@?',\n",
              " 73: 'A10@2',\n",
              " 74: 'A10@1',\n",
              " 75: 'A10@?',\n",
              " 76: 'A11@2',\n",
              " 77: 'A11@1',\n",
              " 78: 'A11@?',\n",
              " 79: 'A12@2',\n",
              " 80: 'A12@1',\n",
              " 81: 'A12@?',\n",
              " 82: 'A13@2',\n",
              " 83: 'A13@?',\n",
              " 84: 'A13@1',\n",
              " 85: 'A14@85',\n",
              " 86: 'A14@135',\n",
              " 87: 'A14@96',\n",
              " 88: 'A14@46',\n",
              " 89: 'A14@?',\n",
              " 90: 'A14@95',\n",
              " 91: 'A14@78',\n",
              " 92: 'A14@59',\n",
              " 93: 'A14@81',\n",
              " 94: 'A14@57',\n",
              " 95: 'A14@72',\n",
              " 96: 'A14@102',\n",
              " 97: 'A14@62',\n",
              " 98: 'A14@53',\n",
              " 99: 'A14@70',\n",
              " 100: 'A14@48',\n",
              " 101: 'A14@133',\n",
              " 102: 'A14@60',\n",
              " 103: 'A14@45',\n",
              " 104: 'A14@175',\n",
              " 105: 'A14@280',\n",
              " 106: 'A14@58',\n",
              " 107: 'A14@67',\n",
              " 108: 'A14@194',\n",
              " 109: 'A14@150',\n",
              " 110: 'A14@180',\n",
              " 111: 'A14@75',\n",
              " 112: 'A14@56',\n",
              " 113: 'A14@71',\n",
              " 114: 'A14@74',\n",
              " 115: 'A14@80',\n",
              " 116: 'A14@191',\n",
              " 117: 'A14@125',\n",
              " 118: 'A14@110',\n",
              " 119: 'A14@50',\n",
              " 120: 'A14@92',\n",
              " 121: 'A14@52',\n",
              " 122: 'A14@26',\n",
              " 123: 'A14@215',\n",
              " 124: 'A14@164',\n",
              " 125: 'A14@103',\n",
              " 126: 'A14@34',\n",
              " 127: 'A14@68',\n",
              " 128: 'A14@82',\n",
              " 129: 'A14@127',\n",
              " 130: 'A14@76',\n",
              " 131: 'A14@100',\n",
              " 132: 'A14@55',\n",
              " 133: 'A14@167',\n",
              " 134: 'A14@30',\n",
              " 135: 'A14@179',\n",
              " 136: 'A14@141',\n",
              " 137: 'A14@44',\n",
              " 138: 'A14@165',\n",
              " 139: 'A14@118',\n",
              " 140: 'A14@230',\n",
              " 141: 'A14@107',\n",
              " 142: 'A14@40',\n",
              " 143: 'A14@147',\n",
              " 144: 'A14@114',\n",
              " 145: 'A14@84',\n",
              " 146: 'A14@123',\n",
              " 147: 'A14@168',\n",
              " 148: 'A14@86',\n",
              " 149: 'A14@138',\n",
              " 150: 'A14@155',\n",
              " 151: 'A14@63',\n",
              " 152: 'A14@256',\n",
              " 153: 'A14@119',\n",
              " 154: 'A14@139',\n",
              " 155: 'A14@90',\n",
              " 156: 'A14@160',\n",
              " 157: 'A14@158',\n",
              " 158: 'A14@115',\n",
              " 159: 'A14@243',\n",
              " 160: 'A14@181',\n",
              " 161: 'A14@130',\n",
              " 162: 'A14@166',\n",
              " 163: 'A14@295',\n",
              " 164: 'A14@120',\n",
              " 165: 'A14@65',\n",
              " 166: 'A14@109',\n",
              " 167: 'A14@89',\n",
              " 168: 'A14@126',\n",
              " 169: 'A15@18',\n",
              " 170: 'A15@42',\n",
              " 171: 'A15@32',\n",
              " 172: 'A15@52',\n",
              " 173: 'A15@200',\n",
              " 174: 'A15@28',\n",
              " 175: 'A15@?',\n",
              " 176: 'A15@48',\n",
              " 177: 'A15@120',\n",
              " 178: 'A15@30',\n",
              " 179: 'A15@249',\n",
              " 180: 'A15@60',\n",
              " 181: 'A15@144',\n",
              " 182: 'A15@89',\n",
              " 183: 'A15@53',\n",
              " 184: 'A15@166',\n",
              " 185: 'A15@20',\n",
              " 186: 'A15@98',\n",
              " 187: 'A15@63',\n",
              " 188: 'A15@46',\n",
              " 189: 'A15@55',\n",
              " 190: 'A15@25',\n",
              " 191: 'A15@58',\n",
              " 192: 'A15@29',\n",
              " 193: 'A15@92',\n",
              " 194: 'A15@150',\n",
              " 195: 'A15@68',\n",
              " 196: 'A15@14',\n",
              " 197: 'A15@16',\n",
              " 198: 'A15@90',\n",
              " 199: 'A15@86',\n",
              " 200: 'A15@110',\n",
              " 201: 'A15@80',\n",
              " 202: 'A15@420',\n",
              " 203: 'A15@44',\n",
              " 204: 'A15@65',\n",
              " 205: 'A15@145',\n",
              " 206: 'A15@31',\n",
              " 207: 'A15@78',\n",
              " 208: 'A15@59',\n",
              " 209: 'A15@38',\n",
              " 210: 'A15@75',\n",
              " 211: 'A15@64',\n",
              " 212: 'A15@54',\n",
              " 213: 'A15@43',\n",
              " 214: 'A15@33',\n",
              " 215: 'A15@15',\n",
              " 216: 'A15@39',\n",
              " 217: 'A15@182',\n",
              " 218: 'A15@271',\n",
              " 219: 'A15@45',\n",
              " 220: 'A15@100',\n",
              " 221: 'A15@242',\n",
              " 222: 'A15@24',\n",
              " 223: 'A15@224',\n",
              " 224: 'A15@69',\n",
              " 225: 'A15@156',\n",
              " 226: 'A15@123',\n",
              " 227: 'A15@117',\n",
              " 228: 'A15@157',\n",
              " 229: 'A15@128',\n",
              " 230: 'A15@23',\n",
              " 231: 'A15@40',\n",
              " 232: 'A15@227',\n",
              " 233: 'A15@269',\n",
              " 234: 'A15@34',\n",
              " 235: 'A15@648',\n",
              " 236: 'A15@225',\n",
              " 237: 'A15@136',\n",
              " 238: 'A15@81',\n",
              " 239: 'A15@153',\n",
              " 240: 'A15@118',\n",
              " 241: 'A15@231',\n",
              " 242: 'A15@101',\n",
              " 243: 'A15@278',\n",
              " 244: 'A15@49',\n",
              " 245: 'A15@181',\n",
              " 246: 'A15@140',\n",
              " 247: 'A15@70',\n",
              " 248: 'A15@114',\n",
              " 249: 'A15@173',\n",
              " 250: 'A15@528',\n",
              " 251: 'A15@152',\n",
              " 252: 'A15@142',\n",
              " 253: 'A15@19',\n",
              " 254: 'A16@4.0',\n",
              " 255: 'A16@3.5',\n",
              " 256: 'A16@?',\n",
              " 257: 'A16@4.4',\n",
              " 258: 'A16@3.9',\n",
              " 259: 'A16@3.7',\n",
              " 260: 'A16@4.9',\n",
              " 261: 'A16@2.9',\n",
              " 262: 'A16@4.3',\n",
              " 263: 'A16@4.1',\n",
              " 264: 'A16@4.2',\n",
              " 265: 'A16@4.7',\n",
              " 266: 'A16@3.8',\n",
              " 267: 'A16@2.7',\n",
              " 268: 'A16@4.6',\n",
              " 269: 'A16@5.0',\n",
              " 270: 'A16@3.3',\n",
              " 271: 'A16@4.5',\n",
              " 272: 'A16@3.4',\n",
              " 273: 'A16@3.1',\n",
              " 274: 'A16@3.0',\n",
              " 275: 'A16@2.6',\n",
              " 276: 'A16@5.3',\n",
              " 277: 'A16@4.8',\n",
              " 278: 'A16@2.8',\n",
              " 279: 'A16@3.6',\n",
              " 280: 'A16@2.1',\n",
              " 281: 'A16@6.4',\n",
              " 282: 'A16@2.4',\n",
              " 283: 'A16@2.2',\n",
              " 284: 'A17@?',\n",
              " 285: 'A17@80',\n",
              " 286: 'A17@75',\n",
              " 287: 'A17@85',\n",
              " 288: 'A17@54',\n",
              " 289: 'A17@52',\n",
              " 290: 'A17@78',\n",
              " 291: 'A17@46',\n",
              " 292: 'A17@63',\n",
              " 293: 'A17@62',\n",
              " 294: 'A17@64',\n",
              " 295: 'A17@39',\n",
              " 296: 'A17@100',\n",
              " 297: 'A17@47',\n",
              " 298: 'A17@70',\n",
              " 299: 'A17@36',\n",
              " 300: 'A17@40',\n",
              " 301: 'A17@74',\n",
              " 302: 'A17@60',\n",
              " 303: 'A17@73',\n",
              " 304: 'A17@90',\n",
              " 305: 'A17@21',\n",
              " 306: 'A17@77',\n",
              " 307: 'A17@29',\n",
              " 308: 'A17@41',\n",
              " 309: 'A17@66',\n",
              " 310: 'A17@57',\n",
              " 311: 'A17@56',\n",
              " 312: 'A17@76',\n",
              " 313: 'A17@58',\n",
              " 314: 'A17@84',\n",
              " 315: 'A17@38',\n",
              " 316: 'A17@67',\n",
              " 317: 'A17@31',\n",
              " 318: 'A17@51',\n",
              " 319: 'A17@23',\n",
              " 320: 'A17@72',\n",
              " 321: 'A17@32',\n",
              " 322: 'A17@30',\n",
              " 323: 'A17@0',\n",
              " 324: 'A17@50',\n",
              " 325: 'A17@43',\n",
              " 326: 'A17@35',\n",
              " 327: 'A17@48',\n",
              " 328: 'A17@42',\n",
              " 329: 'A18@1',\n",
              " 330: 'A18@2'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv_dl9RS3s1F"
      },
      "source": [
        "mapping = {\n",
        "    \n",
        "    'A10':'SPLEEN PALPABLE',\n",
        "    'A11':'SPIDERS',\n",
        "    'A12':'ASCITES',\n",
        "    'A13':'VARICES',\n",
        "    'A14':'ALK PHOSPHATE',\n",
        "    'A15':'SGOT',\n",
        "    'A16':'ALBUMIN',\n",
        "    'A17':'PROTIME',\n",
        "    'A18':'HISTOLOGY',\n",
        "    \n",
        "\t'A0':'class',\n",
        "    'A1':'AGE',\n",
        "    'A2':'SEX',\n",
        "    'A3':'STEROID',\n",
        "    'A4':'ANTIVIRALS',\n",
        "    'A5':'FATIGUE',\n",
        "    'A6':'MALAISE',\n",
        "    'A7':'ANOREXIA',\n",
        "    'A8':'LIVER BIG',\n",
        "    'A9':'LIVER FIRM'\n",
        "           \n",
        "           }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi2_-x6T4SoV"
      },
      "source": [
        "#min_support = np.arange(0.01, .11, 0.01)\n",
        "min_support = [.01]\n",
        "d= \"/content/\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXluIZqe44dN"
      },
      "source": [
        "def parse_input(filename):\n",
        "                with open(filename) as f:\n",
        "                    data = [set(literal_eval(line)) for line in f]\n",
        "                return data"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7u_vqchHI6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75951eda-0763-4ad6-bbee-a7add93134f4"
      },
      "source": [
        "import os\n",
        "from ast import literal_eval\n",
        "\n",
        "for i in (min_support):\n",
        "          print(round(i,2))\n",
        "          print(str(round(i,2)).replace('0.', ''))\n",
        "          file_to_process = d+\"outputHepFP\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          superset_file = d+\"superset\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          superset_file_final = d+\"superset\"+str(round(i,2)).replace('0.', '')+\"_final\"+\".txt\"\n",
        "          superset_file_final_v1 = d+\"superset\"+str(round(i,2)).replace('0.', '')+\"_final_v1\"+\".txt\"\n",
        "          rule_file = d+\"output_rules\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          rule_file_final = d+\"output_rules_final_\"+str(round(i,2)).replace('0.', '')+\".txt\"\n",
        "          \n",
        "          \n",
        "          f = open(file_to_process,'r')\n",
        "          lst = []\n",
        "          for line in f:\n",
        "              line  = line.split(\"SUP\", 1)\n",
        "        \n",
        "              line  = line[0]\n",
        "              lst.append(line)\n",
        "          f.close()\n",
        "          f = open(file_to_process,'w')\n",
        "          for line in lst:\n",
        "              f.write(line+'\\n')\n",
        "          f.close()\n",
        "\n",
        "          f = open(file_to_process,'r')\n",
        "          lst = []\n",
        "          for line in f:\n",
        "              line  = line.split(\"#\", 1)\n",
        "        \n",
        "              line  = line[0]\n",
        "              lst.append(line)\n",
        "          f.close()\n",
        "          f = open(file_to_process,'w')\n",
        "          for line in lst:\n",
        "              f.write(line+'\\n')\n",
        "          f.close()\n",
        "\n",
        "          rpp_process = pd.read_csv(file_to_process, header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          temp_df = pd.concat([rpp_process[['itemsets']], rpp_process['itemsets'].str.split(' ', expand=True)], axis=1)\n",
        "\n",
        "          temp_df=temp_df.drop('itemsets', axis=1)\n",
        "\n",
        "        #for class , moving the class column as first\n",
        "          with open(\"/content/temp_file.csv\", 'w') as f:\n",
        "                for row in temp_df.itertuples(index=False):\n",
        "                    #print(row)\n",
        "                    ls = list(row)\n",
        "                    ls = [x for x in ls if x]\n",
        "                    try:\n",
        "                      if '0' or '1' in ls:\n",
        "                            old_index = ls.index('0') if '0' in ls else ls.index('1')\n",
        "                            #print(\"old_index\", old_index)\n",
        "                            \n",
        "                            ls.insert(0, ls.pop(old_index))\n",
        "                            #print(\"after row\", ls)\n",
        "                            f.write(str(ls)+'\\n')\n",
        "                    except ValueError:\n",
        "                      pass\n",
        "\n",
        "          def compute_output(output_file, data, filter_value):\n",
        "              ls = data\n",
        "              index_to_pop=[]\n",
        "              for set1 in ls:\n",
        "                #print(\"set1\", set1)\n",
        "                for set2 in ls:\n",
        "                    if set1 is set2:\n",
        "                        # Do not try to compare a row with itself\n",
        "                        continue\n",
        "                    elif 1==2:\n",
        "                          if set1 in ls:\n",
        "                            index = ls.index(set1)\n",
        "                            index_to_pop.append(index)\n",
        "\n",
        "                            break\n",
        "              #print(\"Final index_to_pop\",index_to_pop)              \n",
        "              for index in sorted(index_to_pop, reverse=True):\n",
        "                  del ls[index]           \n",
        "              #print(\"Final list\",ls)\n",
        "\n",
        "              f = open(superset_file,'w')\n",
        "              for line in ls:\n",
        "                f.write(str(line)+'\\n')\n",
        "              f.close()\n",
        "\n",
        "          def filter_file(path, filter_value=3, in_name='temp_file.csv', out_name='filteredSets'):\n",
        "              data = parse_input(os.path.join(path, in_name))\n",
        "              #print(\"data\", data)\n",
        "              output_filename = os.path.join(path, '{}{}'.format(out_name, filter_value))\n",
        "              with open(output_filename, 'w') as out_file:\n",
        "                  compute_output(out_file, data, filter_value)\n",
        "\n",
        "          filter_file('/content')\n",
        "\n",
        "          print(\"longest ones sorted\")\n",
        "\n",
        "          with open(superset_file) as filein, open(superset_file_final,'w') as fileout:\n",
        "                    for line in filein:\n",
        "                            line=line.replace(\"'\",\"\")\n",
        "                            line=line.replace(\"}\",\"\")\n",
        "                            line=line.replace(\"{\",\"\")\n",
        "                            line=line.replace(\", \",\" \")\n",
        "                            fileout.write(line)\n",
        "\n",
        "            \n",
        "          with open(superset_file_final) as filein, open(superset_file_final_v1,'w') as fileout:\n",
        "              for line in filein:\n",
        "                  line=line.replace(\"[\",\"\")\n",
        "                  line=line.replace(\"]\",\"\")\n",
        "                  fileout.write(line)\n",
        "\n",
        "          rpp_process = pd.read_csv(superset_file_final_v1, header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          temp_df = pd.concat([rpp_process[['itemsets']], rpp_process['itemsets'].str.split(' ', expand=True)], axis=1)\n",
        "\n",
        "          temp_df=temp_df.drop('itemsets', axis=1)\n",
        "\n",
        "          with open(superset_file, 'w') as f:\n",
        "                  for row in temp_df.itertuples(index=False):\n",
        "                      ls = list(row)\n",
        "                      ls = [x for x in ls if x]\n",
        "                      try:\n",
        "                        if '0' or '1' in ls:\n",
        "                              old_index = ls.index('0') if '0' in ls else ls.index('1')\n",
        "                              #print(\"old_index\", old_index)\n",
        "                              \n",
        "                              ls.insert(0, ls.pop(old_index))\n",
        "                              #print(\"after row\", ls)\n",
        "                              f.write(str(ls)+'\\n')\n",
        "                      except ValueError:\n",
        "                        pass\n",
        "\n",
        "          with open(superset_file) as filein, open(superset_file_final,'w') as fileout:\n",
        "                    for line in filein:\n",
        "                            line=line.replace(\"'\",\"\")\n",
        "                            line=line.replace(\"}\",\"\")\n",
        "                            line=line.replace(\"{\",\"\")\n",
        "                            line=line.replace(\", \",\" \")\n",
        "                            fileout.write(line)\n",
        "\n",
        "            \n",
        "          with open(superset_file_final) as filein, open(superset_file_final_v1,'w') as fileout:\n",
        "              for line in filein:\n",
        "                  line=line.replace(\"[\",\"\")\n",
        "                  line=line.replace(\"]\",\"\")\n",
        "                  fileout.write(line)\n",
        "\n",
        "          superset = pd.read_csv(superset_file_final_v1,header=None, names=[\"itemsets\"],index_col=False)\n",
        "\n",
        "          superset_df = pd.concat([superset[['itemsets']], superset['itemsets'].str.split(' ', expand=True)], axis=1\n",
        "                  )\n",
        "            \n",
        "          superset_df.to_csv(\"superset_df.csv\", header=None, index=False)\n",
        "\n",
        "          print(\"Superset is done\")\n",
        "\n",
        "            \n",
        "            #superset_df = superset_df.loc[superset_df.iloc[:,1].isin(['1','0'])] #filtering rows\n",
        "\n",
        "          #superset_df = superset_df.loc[superset_df.iloc[:,1].isin(['1'])] #filtering only for minority\n",
        "\n",
        "          superset_df.to_csv(\"superset_df_1.csv\", header=None, index=False)\n",
        "\n",
        "          superset_df=superset_df.drop('itemsets', axis=1)\n",
        "\n",
        "          print(\"lenght of columns\", len(superset_df.columns))\n",
        "          if len(superset_df.columns) ==9:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g', 'h', 'i']\n",
        "          elif len(superset_df.columns) ==8:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g', 'h']\n",
        "\n",
        "          elif len(superset_df.columns) ==7:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f','g']\n",
        "\n",
        "          elif len(superset_df.columns) ==6:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e', 'f']\n",
        "\n",
        "          elif len(superset_df.columns) ==5:\n",
        "              superset_df.columns = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "          for i,n in enumerate(superset_df.columns):\n",
        "  \n",
        "                    superset_df[n] = '@' + superset_df[n].astype(str)\n",
        "\n",
        "          di = {f'@{k}': v for k, v in my_dict.items()}\n",
        "\n",
        "            \n",
        "          for col in superset_df:\n",
        "              superset_df[col] = superset_df[col].replace(di)\n",
        "\n",
        "          print(\"Mapping is done\")\n",
        "          superset_df.to_csv(\"superset_df_1_after_mapping.csv\", header=None, index=False) \n",
        "          if len(superset_df.columns) ==9:\n",
        "\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']+','+superset_df['h']+','+superset_df['i']\n",
        "          elif len(superset_df.columns) ==8:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']+','+superset_df['h']\n",
        "          \n",
        "          elif len(superset_df.columns) ==7:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']+','+superset_df['g']\n",
        "          \n",
        "          elif len(superset_df.columns) ==6:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']+','+superset_df['f']\n",
        "          \n",
        "          elif len(superset_df.columns) ==5:\n",
        "              superset_df['rules'] = superset_df['b']+','+superset_df['c']+','+superset_df['d']+','+superset_df['e']\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          superset_df['pos'] = superset_df['rules'].str.find('@None')\n",
        "          superset_df['rules'] = superset_df.apply(lambda x: x['rules'][0:x['pos']],axis=1)\n",
        "          superset_df['rules'] = superset_df['rules']+\">\"+superset_df['a']\n",
        "          superset_df['rules'] = superset_df['rules'].str.replace(',@,','')\n",
        "          superset_df['rules'] = superset_df['rules'].str.replace(',>','>')\n",
        "\n",
        "          superset_df['rules'].to_csv(rule_file_final, header=None, index=False)\n",
        "          #for classifer file\n",
        "          superset_df['rules'].to_csv(rule_file, header=None, index=False)\n",
        "\n",
        "          print(\"Rules generated\")\n",
        "          text = rule_file_final\n",
        "          fields = mapping\n",
        "\n",
        "          lst = []\n",
        "          for line in fileinput.input(text):\n",
        "              for field in fields:\n",
        "                  if field in line:\n",
        "                      line = line.replace(field, fields[field])\n",
        "              lst.append(line)\n",
        "\n",
        "          print(lst)\n",
        "\n",
        "          f = open(rule_file_final,'w')\n",
        "          for line in lst:\n",
        "              f.write(line)\n",
        "          f.close()\n",
        "\n",
        "          print(\"Rules mapping done\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01\n",
            "01\n",
            "longest ones sorted\n",
            "Superset is done\n",
            "lenght of columns 6\n",
            "Mapping is done\n",
            "Rules generated\n",
            "['SEX@1>class@1\\n', '\"SPIDERS@1,SEX@1>class@1\"\\n', '\"SPIDERS@1,HISTOLOGY@2,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,SPIDERS@1,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,SPIDERS@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"SPIDERS@1,FATIGUE@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,LIVER BIG@2,SPIDERS@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,SPIDERS@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"SPIDERS@1,LIVER BIG@2,SEX@1>class@1\"\\n', '\"LIVER BIG@2,SPIDERS@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"SPIDERS@1,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"FATIGUE@1,MALAISE@1,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,MALAISE@1,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,MALAISE@1,SEX@1,ANTIVIRALS@>class@1\"\\n', '\"FATIGUE@1,MALAISE@1,SEX@1,STEROID@1>class@1\"\\n', '\"FATIGUE@1,LIVER BIG@2,MALAISE@1,SEX@1>class@1\"\\n', '\"SPLEEN PALPABLE@2,FATIGUE@1,MALAISE@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,MALAISE@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"HISTOLOGY@2,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,STEROID@1,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,LIVER BIG@2,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,LIVER BIG@2,SEX@1,ANTIVIRALS@>class@1\"\\n', '\"HISTOLOGY@2,FATIGUE@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"LIVER BIG@2,HISTOLOGY@2,SEX@1>class@1\"\\n', '\"HISTOLOGY@2,LIVER BIG@2,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"HISTOLOGY@2,ANOREXIA@2,SEX@1>class@1\"\\n', '\"ANOREXIA@2,HISTOLOGY@2,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"HISTOLOGY@2,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"STEROID@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,STEROID@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,SEX@1,ANTIVIRALS@2,STEROID@1>class@1\"\\n', '\"ANTIVIRALS@2,STEROID@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,SEX@1>class@1\"\\n', '\"LIVER BIG@2,FATIGUE@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,LIVER BIG@2,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"SPLEEN PALPABLE@2,FATIGUE@1,SEX@1>class@1\"\\n', '\"FATIGUE@1,ANOREXIA@2,SEX@1>class@1\"\\n', '\"ANOREXIA@2,FATIGUE@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"FATIGUE@1,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"VARICES@2,FATIGUE@1,SEX@1,ANTIVIRALS@2>class@1\"\\n', '\"VARICES@2,FATIGUE@1,SEX@1>class@1\"\\n', '\"LIVER BIG@2,SEX@1>class@1\"\\n', '\"LIVER BIG@2,ANOREXIA@2,SEX@1>class@1\"\\n', '\"LIVER BIG@2,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"SPLEEN PALPABLE@2,SEX@1>class@1\"\\n', '\"SPLEEN PALPABLE@2,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"ANOREXIA@2,SEX@1>class@1\"\\n', '\"ANTIVIRALS@2,ANOREXIA@2,SEX@1>class@1\"\\n', '\"ASCITES@2,SEX@1>class@1\"\\n', '\"ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"VARICES@2,ANTIVIRALS@2,SEX@1>class@1\"\\n', '\"VARICES@2,SEX@1>class@1\"\\n']\n",
            "Rules mapping done\n"
          ]
        }
      ]
    }
  ]
}